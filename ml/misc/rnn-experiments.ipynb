{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/neilkloot/anaconda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from timeit import default_timer as timer\n",
    "from datetime import datetime, timedelta\n",
    "import numba\n",
    "from ensemble_processing import load_data, load, save\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows=500\n",
    "pd.options.display.max_columns=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_week_median(df):\n",
    "    # assumes a datetime index\n",
    "    return df.resample('1W').median()\n",
    "\n",
    "\n",
    "# test_df = pd.DataFrame({\n",
    "#     'dates': ['2018-01-02', '2018-01-03', '2018-01-04', '2018-03-05', '2019-04-09'],\n",
    "#     'val': [1, 2, 2, 3, 3],\n",
    "#     'date2': ['2019-01-02', '2019-01-03','2019-02-02', '2019-02-03', '2019-05-05']\n",
    "# })\n",
    "# test_df['dates'] = pd.to_datetime(test_df['dates'], errors='coerce')\n",
    "# test_df['date2'] = pd.to_datetime(test_df['date2'], errors='coerce')\n",
    "# test_df.sort_values(by=['dates'], inplace=True)\n",
    "# print(test_df.dtypes)\n",
    "# test_df.set_index('dates', inplace=True)\n",
    "# print(test_df)\n",
    "# print(test_df.index)\n",
    "# print(test_df.index.week, test_df.index.year)\n",
    "# return_week_median(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.20.3'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "print('Loading data...')\n",
    "all_df = pd.read_pickle('../data/ml-20190115-processed.pkl.gz', compression='gzip')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create one vector with all symbols per week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "# experiment_symbols = ['WAX', 'IVV', 'WESN', 'IAA', 'XRO', 'MTB', 'MXR', 'RCP', 'ISX', 'IMI']\n",
    "# experiment_symbols = ['GSW','CUA','FLT','OVN','FDM','SVW','FPH','JHC','TRS','SGP','DOW','AOG',\n",
    "#                       'HSN','SWM','CMI','BCI','SWJ','MYX','CZA','VMT','CLQ','CSE','OOK','BRL',\n",
    "#                       'ENC','SIO','ECG','MHC','ICT','NTM','REV','FND','GOE','QMN','DSB','KRS',\n",
    "#                       'LAU','GSZ','SAS','OCC','INK','EVM','XST','WWI','BSM','TKL','ZMI','DGO',\n",
    "#                       'ALT','RNO','IRC','AYM','IVG','MPE','OAR','DSE','KTL']\n",
    "\n",
    "experiment_symbols = ['GSW','VGAD','CUA','TIX','FLT','PTN','OVN','IRU','FDM','HDF','SVW','BWX',\n",
    "                      'FPH','AGL','JHC','HIN','TRS','HGO','SGP','AVN','DOW','NVL','AOG','BGL',\n",
    "                      'HSN','CSS','SWM','RIC','CMI','MKE','BCI','IBG','SWJ','IXP','MYX','RNY',\n",
    "                      'CZA','SRV','VMT','CDM','CLQ','BPS','CSE','ORR','OOK','EOS','BRL','BWP',\n",
    "                      'ENC','FCT','SIO','SZG','ECG','SXA','MHC','PHK','ICT','CLH','NTM','TAS',\n",
    "                      'REV','CII','FND','ZGM','GOE','SSI','QMN','EAL','DSB','PNX','KRS','RXH',\n",
    "                      'LAU','CNW','GSZ','IBC','SAS','EAS','OCC','AIS','INK','AIK','EVM','MSV',\n",
    "                      'XST','GMR','WWI','JYC','BSM','VRX','TKL','WFE','ZMI','SHK','DGO','BD1',\n",
    "                      'ALT','SES','RNO','MXC','IRC','GTR','AYM','RLC','IVG','MDG','MPE','MOT',\n",
    "                      'OAR','HGL','DSE','VII','KTL']\n",
    "\n",
    "WHOLE_MARKET_COLUMNS = ['quoteDate', 'allordpreviousclose', 'allorddayshigh', 'allorddayslow', 'asxpreviousclose',\n",
    "                        'asxdayshigh', 'asxdayslow', '640106_A3597525W', 'FIRMMCRT', 'FXRUSD', 'GRCPAIAD',\n",
    "                        'GRCPAISAD', 'GRCPBCAD', 'GRCPBCSAD', 'GRCPBMAD', 'GRCPNRAD', 'GRCPRCAD', 'H01_GGDPCVGDP',\n",
    "                        'H01_GGDPCVGDPFY', 'H05_GLFSEPTPOP']\n",
    "\n",
    "SYMBOL_COLS = ['quoteDate', 'adjustedPrice', 'AINTCOV', 'Beta', 'BookValuePerShareYear', 'CashPerShareYear', \n",
    "               'DPSRecentYear', 'EPS', 'Float', 'MarketCap', 'OperatingMargin', 'PE', 'ReturnOnEquityYear', \n",
    "               'TotalDebtToEquityYear', 'exDividendDate']\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data loaded 5384641 records\n",
      "Records being used 268358\n",
      "Min date for recordset 2007-07-02 00:00:00\n",
      "Max date for recordset 2019-01-14 00:00:00\n",
      "Number of weeks in dataset 603\n",
      "Symbol: GSW number of daily records: 593\n",
      "Symbol: GSW number of median records: 111 average change: 1.5623096227645874\n",
      "----------\n",
      "Symbol: VGAD number of daily records: 911\n",
      "Symbol: VGAD number of median records: 181 average change: 0.14820389449596405\n",
      "----------\n",
      "Symbol: CUA number of daily records: 794\n",
      "Symbol: CUA number of median records: 164 average change: 0.22959904372692108\n",
      "----------\n",
      "Symbol: TIX number of daily records: 1049\n",
      "Symbol: TIX number of median records: 216 average change: 0.32570168375968933\n",
      "----------\n",
      "Symbol: FLT number of daily records: 3009\n",
      "Symbol: FLT number of median records: 603 average change: 0.39136746525764465\n",
      "----------\n",
      "Symbol: PTN number of daily records: 764\n",
      "Symbol: PTN number of median records: 157 average change: -1.4026567935943604\n",
      "----------\n",
      "Symbol: OVN number of daily records: 697\n",
      "Symbol: OVN number of median records: 131 average change: -0.5754274129867554\n",
      "----------\n",
      "Symbol: IRU number of daily records: 2710\n",
      "Symbol: IRU number of median records: 547 average change: 0.20797985792160034\n",
      "----------\n",
      "Symbol: FDM number of daily records: 561\n",
      "Symbol: FDM number of median records: 103 average change: 0.7777363061904907\n",
      "----------\n",
      "Symbol: HDF number of daily records: 1377\n",
      "Symbol: HDF number of median records: 281 average change: 0.4488772749900818\n",
      "----------\n",
      "Symbol: SVW number of daily records: 3009\n",
      "Symbol: SVW number of median records: 603 average change: 0.28480011224746704\n",
      "----------\n",
      "Symbol: BWX number of daily records: 868\n",
      "Symbol: BWX number of median records: 167 average change: -0.023402713239192963\n",
      "----------\n",
      "Symbol: FPH number of daily records: 3009\n",
      "Symbol: FPH number of median records: 603 average change: 0.3575170636177063\n",
      "----------\n",
      "Symbol: AGL number of daily records: 3009\n",
      "Symbol: AGL number of median records: 603 average change: 0.18154162168502808\n",
      "----------\n",
      "Symbol: JHC number of daily records: 1266\n",
      "Symbol: JHC number of median records: 249 average change: -0.19374187290668488\n",
      "----------\n",
      "Symbol: HIN number of daily records: 1588\n",
      "Symbol: HIN number of median records: 324 average change: 0.6466729044914246\n",
      "----------\n",
      "Symbol: TRS number of daily records: 3009\n",
      "Symbol: TRS number of median records: 603 average change: -0.019433580338954926\n",
      "----------\n",
      "Symbol: HGO number of daily records: 3006\n",
      "Symbol: HGO number of median records: 602 average change: -0.4038379490375519\n",
      "----------\n",
      "Symbol: SGP number of daily records: 3009\n",
      "Symbol: SGP number of median records: 603 average change: 0.08218517899513245\n",
      "----------\n",
      "Symbol: AVN number of daily records: 886\n",
      "Symbol: AVN number of median records: 171 average change: 0.12115392088890076\n",
      "----------\n",
      "Symbol: DOW number of daily records: 3009\n",
      "Symbol: DOW number of median records: 603 average change: 0.2722701132297516\n",
      "----------\n",
      "Symbol: NVL number of daily records: 931\n",
      "Symbol: NVL number of median records: 180 average change: 0.3226762115955353\n",
      "----------\n",
      "Symbol: AOG number of daily records: 3009\n",
      "Symbol: AOG number of median records: 603 average change: -0.16549302637577057\n",
      "----------\n",
      "Symbol: BGL number of daily records: 2417\n",
      "Symbol: BGL number of median records: 493 average change: 0.608327329158783\n",
      "----------\n",
      "Symbol: HSN number of daily records: 3009\n",
      "Symbol: HSN number of median records: 603 average change: 0.9200230836868286\n",
      "----------\n",
      "Symbol: CSS number of daily records: 3009\n",
      "Symbol: CSS number of median records: 603 average change: 1.4186911582946777\n",
      "----------\n",
      "Symbol: SWM number of daily records: 3009\n",
      "Symbol: SWM number of median records: 603 average change: -0.20732101798057556\n",
      "----------\n",
      "Symbol: RIC number of daily records: 3009\n",
      "Symbol: RIC number of median records: 603 average change: 0.1724822074174881\n",
      "----------\n",
      "Symbol: CMI number of daily records: 3009\n",
      "Symbol: CMI number of median records: 603 average change: 0.4208458662033081\n",
      "----------\n",
      "Symbol: MKE number of daily records: 1368\n",
      "Symbol: MKE number of median records: 279 average change: 0.09567049145698547\n",
      "----------\n",
      "Symbol: BCI number of daily records: 3009\n",
      "Symbol: BCI number of median records: 603 average change: 0.14463169872760773\n",
      "----------\n",
      "Symbol: IBG number of daily records: 3007\n",
      "Symbol: IBG number of median records: 602 average change: -0.3113193213939667\n",
      "----------\n",
      "Symbol: SWJ number of daily records: 2021\n",
      "Symbol: SWJ number of median records: 403 average change: -0.02307419292628765\n",
      "----------\n",
      "Symbol: IXP number of daily records: 2326\n",
      "Symbol: IXP number of median records: 468 average change: 0.14531956613063812\n",
      "----------\n",
      "Symbol: MYX number of daily records: 3009\n",
      "Symbol: MYX number of median records: 603 average change: 0.4070049226284027\n",
      "----------\n",
      "Symbol: RNY number of daily records: 3009\n",
      "Symbol: RNY number of median records: 603 average change: -0.32572799921035767\n",
      "----------\n",
      "Symbol: CZA number of daily records: 2676\n",
      "Symbol: CZA number of median records: 544 average change: -0.2901212275028229\n",
      "----------\n",
      "Symbol: SRV number of daily records: 3009\n",
      "Symbol: SRV number of median records: 603 average change: 0.060784101486206055\n",
      "----------\n",
      "Symbol: VMT number of daily records: 3007\n",
      "Symbol: VMT number of median records: 602 average change: -0.2355944663286209\n",
      "----------\n",
      "Symbol: CDM number of daily records: 3009\n",
      "Symbol: CDM number of median records: 603 average change: 1.9711366891860962\n",
      "----------\n",
      "Symbol: CLQ number of daily records: 2915\n",
      "Symbol: CLQ number of median records: 585 average change: 0.2801859378814697\n",
      "----------\n",
      "Symbol: BPS number of daily records: 944\n",
      "Symbol: BPS number of median records: 188 average change: -0.3235008716583252\n",
      "----------\n",
      "Symbol: CSE number of daily records: 2997\n",
      "Symbol: CSE number of median records: 601 average change: 0.2980007529258728\n",
      "----------\n",
      "Symbol: ORR number of daily records: 1947\n",
      "Symbol: ORR number of median records: 387 average change: 0.2841915488243103\n",
      "----------\n",
      "Symbol: OOK number of daily records: 3007\n",
      "Symbol: OOK number of median records: 602 average change: -0.3677982687950134\n",
      "----------\n",
      "Symbol: EOS number of daily records: 3008\n",
      "Symbol: EOS number of median records: 603 average change: 0.4749144911766052\n",
      "----------\n",
      "Symbol: BRL number of daily records: 2887\n",
      "Symbol: BRL number of median records: 579 average change: 0.19327324628829956\n",
      "----------\n",
      "Symbol: BWP number of daily records: 3009\n",
      "Symbol: BWP number of median records: 603 average change: 0.24145127832889557\n",
      "----------\n",
      "Symbol: ENC number of daily records: 680\n",
      "Symbol: ENC number of median records: 134 average change: 0.5996149182319641\n",
      "----------\n",
      "Symbol: FCT number of daily records: 2020\n",
      "Symbol: FCT number of median records: 401 average change: -0.5159421563148499\n",
      "----------\n",
      "Symbol: SIO number of daily records: 1118\n",
      "Symbol: SIO number of median records: 218 average change: -0.3925393223762512\n",
      "----------\n",
      "Symbol: SZG number of daily records: 1922\n",
      "Symbol: SZG number of median records: 392 average change: -1.018168330192566\n",
      "----------\n",
      "Symbol: ECG number of daily records: 1106\n",
      "Symbol: ECG number of median records: 216 average change: 0.8595371842384338\n",
      "----------\n",
      "Symbol: SXA number of daily records: 1548\n",
      "Symbol: SXA number of median records: 305 average change: -0.6686112284660339\n",
      "----------\n",
      "Symbol: MHC number of daily records: 2861\n",
      "Symbol: MHC number of median records: 572 average change: 0.11714927107095718\n",
      "----------\n",
      "Symbol: PHK number of daily records: 2888\n",
      "Symbol: PHK number of median records: 582 average change: 0.2021724134683609\n",
      "----------\n",
      "Symbol: ICT number of daily records: 2405\n",
      "Symbol: ICT number of median records: 480 average change: -0.28658026456832886\n",
      "----------\n",
      "Symbol: CLH number of daily records: 3009\n",
      "Symbol: CLH number of median records: 603 average change: 0.30836930871009827\n",
      "----------\n",
      "Symbol: NTM number of daily records: 3009\n",
      "Symbol: NTM number of median records: 603 average change: 0.024643346667289734\n",
      "----------\n",
      "Symbol: TAS number of daily records: 3009\n",
      "Symbol: TAS number of median records: 603 average change: 0.00776966055855155\n",
      "----------\n",
      "Symbol: REV number of daily records: 842\n",
      "Symbol: REV number of median records: 162 average change: -0.8185768127441406\n",
      "----------\n",
      "Symbol: CII number of daily records: 3008\n",
      "Symbol: CII number of median records: 603 average change: 0.6145869493484497\n",
      "----------\n",
      "Symbol: FND number of daily records: 2988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symbol: FND number of median records: 599 average change: 0.007512769661843777\n",
      "----------\n",
      "Symbol: ZGM number of daily records: 2364\n",
      "Symbol: ZGM number of median records: 483 average change: -0.24111390113830566\n",
      "----------\n",
      "Symbol: GOE number of daily records: 2222\n",
      "Symbol: GOE number of median records: 454 average change: -0.1262877881526947\n",
      "----------\n",
      "Symbol: SSI number of daily records: 789\n",
      "Symbol: SSI number of median records: 161 average change: -0.36029860377311707\n",
      "----------\n",
      "Symbol: QMN number of daily records: 2407\n",
      "Symbol: QMN number of median records: 488 average change: -0.04624941945075989\n",
      "----------\n",
      "Symbol: EAL number of daily records: 2400\n",
      "Symbol: EAL number of median records: 491 average change: -0.2165878415107727\n",
      "----------\n",
      "Symbol: DSB number of daily records: 1640\n",
      "Symbol: DSB number of median records: 336 average change: -0.05726109817624092\n",
      "----------\n",
      "Symbol: PNX number of daily records: 2850\n",
      "Symbol: PNX number of median records: 570 average change: -0.12348673492670059\n",
      "----------\n",
      "Symbol: KRS number of daily records: 2873\n",
      "Symbol: KRS number of median records: 579 average change: 0.05154682695865631\n",
      "----------\n",
      "Symbol: RXH number of daily records: 1139\n",
      "Symbol: RXH number of median records: 222 average change: -0.6992692947387695\n",
      "----------\n",
      "Symbol: LAU number of daily records: 3009\n",
      "Symbol: LAU number of median records: 603 average change: 0.27669432759284973\n",
      "----------\n",
      "Symbol: CNW number of daily records: 3009\n",
      "Symbol: CNW number of median records: 603 average change: -0.024736450985074043\n",
      "----------\n",
      "Symbol: GSZ number of daily records: 1821\n",
      "Symbol: GSZ number of median records: 371 average change: 0.1870349794626236\n",
      "----------\n",
      "Symbol: IBC number of daily records: 3004\n",
      "Symbol: IBC number of median records: 602 average change: 0.0003632321022450924\n",
      "----------\n",
      "Symbol: SAS number of daily records: 3009\n",
      "Symbol: SAS number of median records: 603 average change: -0.2795788645744324\n",
      "----------\n",
      "Symbol: EAS number of daily records: 3007\n",
      "Symbol: EAS number of median records: 602 average change: 0.1176319494843483\n",
      "----------\n",
      "Symbol: OCC number of daily records: 1187\n",
      "Symbol: OCC number of median records: 232 average change: -0.10379447042942047\n",
      "----------\n",
      "Symbol: AIS number of daily records: 2082\n",
      "Symbol: AIS number of median records: 415 average change: 0.17266897857189178\n",
      "----------\n",
      "Symbol: INK number of daily records: 3009\n",
      "Symbol: INK number of median records: 603 average change: 1.908996820449829\n",
      "----------\n",
      "Symbol: AIK number of daily records: 2505\n",
      "Symbol: AIK number of median records: 512 average change: -0.2304544448852539\n",
      "----------\n",
      "Symbol: EVM number of daily records: 2264\n",
      "Symbol: EVM number of median records: 463 average change: -0.025182627141475677\n",
      "----------\n",
      "Symbol: MSV number of daily records: 1963\n",
      "Symbol: MSV number of median records: 389 average change: -0.17144586145877838\n",
      "----------\n",
      "Symbol: XST number of daily records: 2994\n",
      "Symbol: XST number of median records: 600 average change: -0.21439673006534576\n",
      "----------\n",
      "Symbol: GMR number of daily records: 3008\n",
      "Symbol: GMR number of median records: 602 average change: 0.024148251861333847\n",
      "----------\n",
      "Symbol: WWI number of daily records: 2886\n",
      "Symbol: WWI number of median records: 578 average change: -0.2356816530227661\n",
      "----------\n",
      "Symbol: JYC number of daily records: 3007\n",
      "Symbol: JYC number of median records: 602 average change: 0.3260161280632019\n",
      "----------\n",
      "Symbol: BSM number of daily records: 3009\n",
      "Symbol: BSM number of median records: 603 average change: -0.1719675213098526\n",
      "----------\n",
      "Symbol: VRX number of daily records: 2073\n",
      "Symbol: VRX number of median records: 413 average change: 0.3042612373828888\n",
      "----------\n",
      "Symbol: TKL number of daily records: 3005\n",
      "Symbol: TKL number of median records: 602 average change: -0.08936168253421783\n",
      "----------\n",
      "Symbol: WFE number of daily records: 1884\n",
      "Symbol: WFE number of median records: 379 average change: -0.3539915680885315\n",
      "----------\n",
      "Symbol: ZMI number of daily records: 2953\n",
      "Symbol: ZMI number of median records: 592 average change: -0.4914781153202057\n",
      "----------\n",
      "Symbol: SHK number of daily records: 3003\n",
      "Symbol: SHK number of median records: 602 average change: -0.1453743577003479\n",
      "----------\n",
      "Symbol: DGO number of daily records: 2872\n",
      "Symbol: DGO number of median records: 576 average change: 9.109335899353027\n",
      "----------\n",
      "Symbol: BD1 number of daily records: 3008\n",
      "Symbol: BD1 number of median records: 603 average change: 0.01868300512433052\n",
      "----------\n",
      "Symbol: ALT number of daily records: 3007\n",
      "Symbol: ALT number of median records: 603 average change: -0.01787378080189228\n",
      "----------\n",
      "Symbol: SES number of daily records: 3009\n",
      "Symbol: SES number of median records: 603 average change: -0.030258409678936005\n",
      "----------\n",
      "Symbol: RNO number of daily records: 2950\n",
      "Symbol: RNO number of median records: 592 average change: 25.261638641357422\n",
      "----------\n",
      "Symbol: MXC number of daily records: 3004\n",
      "Symbol: MXC number of median records: 602 average change: -0.24335135519504547\n",
      "----------\n",
      "Symbol: IRC number of daily records: 3007\n",
      "Symbol: IRC number of median records: 602 average change: 0.11412937194108963\n",
      "----------\n",
      "Symbol: GTR number of daily records: 2978\n",
      "Symbol: GTR number of median records: 597 average change: -0.21547716856002808\n",
      "----------\n",
      "Symbol: AYM number of daily records: 1866\n",
      "Symbol: AYM number of median records: 372 average change: -0.1579514741897583\n",
      "----------\n",
      "Symbol: RLC number of daily records: 3009\n",
      "Symbol: RLC number of median records: 603 average change: -0.1915445774793625\n",
      "----------\n",
      "Symbol: IVG number of daily records: 742\n",
      "Symbol: IVG number of median records: 152 average change: -0.6165124773979187\n",
      "----------\n",
      "Symbol: MDG number of daily records: 2449\n",
      "Symbol: MDG number of median records: 501 average change: -0.10202647745609283\n",
      "----------\n",
      "Symbol: MPE number of daily records: 2702\n",
      "Symbol: MPE number of median records: 548 average change: -0.18417111039161682\n",
      "----------\n",
      "Symbol: MOT number of daily records: 2673\n",
      "Symbol: MOT number of median records: 543 average change: 0.6922990679740906\n",
      "----------\n",
      "Symbol: OAR number of daily records: 2981\n",
      "Symbol: OAR number of median records: 598 average change: -0.3066778779029846\n",
      "----------\n",
      "Symbol: HGL number of daily records: 3006\n",
      "Symbol: HGL number of median records: 602 average change: 0.08442723006010056\n",
      "----------\n",
      "Symbol: DSE number of daily records: 1913\n",
      "Symbol: DSE number of median records: 380 average change: 6.339648723602295\n",
      "----------\n",
      "Symbol: VII number of daily records: 2992\n",
      "Symbol: VII number of median records: 600 average change: 0.3990400433540344\n",
      "----------\n",
      "Symbol: KTL number of daily records: 1703\n",
      "Symbol: KTL number of median records: 347 average change: -0.06028902903199196\n",
      "----------\n",
      "Creating concatenated dataframes\n",
      "Number of symbol median records 53829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/neilkloot/anaconda/lib/python3.6/site-packages/pandas/core/frame.py:3697: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(603, 2621)\n",
      "Num target cols 113\n"
     ]
    }
   ],
   "source": [
    "### 2D vector - flat\n",
    "\n",
    "#### Get unique whole market vals (shared across all symbols)\n",
    "print('Total data loaded', len(all_df), 'records')\n",
    "daily_symbols_df = all_df[all_df['symbol'].isin(experiment_symbols)]\n",
    "\n",
    "print('Records being used', len(daily_symbols_df))\n",
    "\n",
    "# Determine the min and max range for our symbols\n",
    "all_possible_dates = daily_symbols_df['quoteDate'].dropna().drop_duplicates()\n",
    "overall_min_date = all_possible_dates.min()\n",
    "overall_max_date = all_possible_dates.max()\n",
    "print('Min date for recordset', overall_min_date)\n",
    "print('Max date for recordset', overall_max_date)\n",
    "\n",
    "reference_whole_market_data = daily_symbols_df[WHOLE_MARKET_COLUMNS].drop_duplicates()\n",
    "\n",
    "# Ensure there is only one record per day\n",
    "reference_whole_market_data = reference_whole_market_data.groupby(\n",
    "    'quoteDate').first()\n",
    "reference_whole_market_data['quoteDate'] = reference_whole_market_data.index\n",
    "\n",
    "# Convert records to weekly medi    \n",
    "median_whole_market_data = return_week_median(reference_whole_market_data)\n",
    "print('Number of weeks in dataset', len(median_whole_market_data))\n",
    "\n",
    "\n",
    "# Retrieve date parameters for start and finish\n",
    "symbols_median_dfs = []\n",
    "\n",
    "# Create symbol dfs with daily data, the convert to weekly median data\n",
    "for symbol in experiment_symbols:\n",
    "    # Filter to model data for this symbol and re-set the pandas indexes\n",
    "    model_data = daily_symbols_df.loc[daily_symbols_df['symbol'] == symbol][SYMBOL_COLS]\n",
    "    \n",
    "    print('Symbol:', symbol, 'number of daily records:', len(model_data))\n",
    "        \n",
    "#     print('Converting to week median values')    \n",
    "    # Conert date to number\n",
    "    model_data['exDividendRelative'] = model_data['exDividendDate'] - model_data['quoteDate']\n",
    "    # convert string difference value to integer\n",
    "    model_data['exDividendRelative'] = model_data['exDividendRelative'].apply(\n",
    "        lambda x: np.nan if pd.isnull(x) else x.days)\n",
    "    # Make sure it is the minimum data type size\n",
    "    model_data.loc[:, 'exDividendRelative'] = model_data['exDividendRelative'].astype('int32', errors='ignore')\n",
    "        \n",
    "    median_model_data = return_week_median(model_data)\n",
    "    \n",
    "    # Re-add in key categorical values\n",
    "    median_model_data['symbol'] = symbol\n",
    "                \n",
    "    # Calculate weekly change percentage allowing for 0 values\n",
    "    median_model_data['price_change_perc'] = (median_model_data['adjustedPrice'] - \\\n",
    "                                                median_model_data['adjustedPrice'].shift(1)) / \\\n",
    "                                                median_model_data['adjustedPrice'].shift(1).clip(lower=0.1) * \\\n",
    "                                                100\n",
    "\n",
    "    # Calculate target (Y) values for the following 8 weeks\n",
    "    for week_num in range(1, 9):\n",
    "        shift_amount = week_num * -1\n",
    "        median_model_data['y_' + str(week_num)] = (median_model_data['adjustedPrice'] - \\\n",
    "                                                median_model_data['adjustedPrice'].shift(shift_amount)) / \\\n",
    "                                                median_model_data['adjustedPrice'].clip(lower=0.1) * \\\n",
    "                                                100\n",
    "            \n",
    "    print('Symbol:', symbol, 'number of median records:', len(median_model_data), 'average change:', np.mean(median_model_data['price_change_perc']))\n",
    "    print('-'*10)\n",
    "                \n",
    "    symbols_median_dfs.append(median_model_data)\n",
    "    \n",
    "\n",
    "# Create concatenated dataframe with all weekly median data\n",
    "print('Creating concatenated dataframes')\n",
    "symbols_median_df = pd.concat(symbols_median_dfs)\n",
    "\n",
    "print('Number of symbol median records', len(symbols_median_df))\n",
    "# symbols_median_df\n",
    "\n",
    "target_cols = []\n",
    "target_col_names = []\n",
    "\n",
    "# Work through median values for each symbol and to the whole market data or -9999 for no value\n",
    "for symbol in experiment_symbols:\n",
    "    symbol_df = symbols_median_df.loc[symbols_median_df['symbol'] == symbol]\n",
    "#     print(symbol_df.columns)\n",
    "    symbol_df.drop(['symbol'], axis=1, inplace=True)\n",
    "    symbol_df = symbol_df.add_prefix(symbol + '_')\n",
    "    \n",
    "    # Add target column name for future y data\n",
    "    target_cols.append(symbol + '_price_change_perc')\n",
    "    target_col_names.append(symbol)\n",
    "\n",
    "    median_whole_market_data = median_whole_market_data.merge(symbol_df, left_index=True, right_index=True, \n",
    "                                                              how='left', suffixes = ('wm_', '_' + symbol))\n",
    "\n",
    "    median_whole_market_data['month'] = median_whole_market_data.index.month\n",
    "    median_whole_market_data['week_num'] = median_whole_market_data.index.weekofyear\n",
    "    median_whole_market_data['year'] = median_whole_market_data.index.year\n",
    "    \n",
    "#     median_whole_market_data[symbol_df.columns.values] = median_whole_market_data[symbol_df.columns.values].fillna(-999)\n",
    "    median_whole_market_data.fillna(-999, inplace=True)\n",
    "\n",
    "    \n",
    "print(median_whole_market_data.shape)\n",
    "print('Num target cols', len(target_cols))\n",
    "\n",
    "# Construct set of cols which are the future 8 week targets\n",
    "eight_week_target_cols = []\n",
    "for symbol in target_col_names:\n",
    "    for week_num in range(1, 9):\n",
    "        eight_week_target_cols.append(symbol + '_y_' + str(week_num))\n",
    "    \n",
    "# Create target series and remove target cols from X data \n",
    "whole_market_targets = median_whole_market_data[eight_week_target_cols]\n",
    "\n",
    "median_whole_market_data.drop(eight_week_target_cols, axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['allordpreviousclose', 'allorddayshigh', 'allorddayslow',\n",
       "       'asxpreviousclose', 'asxdayshigh', 'asxdayslow', '640106_A3597525W',\n",
       "       'FIRMMCRT', 'FXRUSD', 'GRCPAIAD',\n",
       "       ...\n",
       "       'KTL_exDividendRelative', 'KTL_price_change_perc', 'KTL_y_1', 'KTL_y_2',\n",
       "       'KTL_y_3', 'KTL_y_4', 'KTL_y_5', 'KTL_y_6', 'KTL_y_7', 'KTL_y_8'],\n",
       "      dtype='object', length=2621)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "median_whole_market_data.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### X - 3D vector - date, symbol, features\n",
    "### y - 2D vector - date, symbol_1 val, symbol_2 val ... \n",
    "\n",
    "### 2D vector - flat\n",
    "\n",
    "# ConvLSTM2D expects input as [samples, timesteps, rows, columns, features]\n",
    "#                             [sample-> 16 input weeks -> 1 -> whole market vals + symbol vals features]\n",
    "\n",
    "#### Get unique whole market vals (shared across all symbols)\n",
    "print('Total data loaded', len(all_df), 'records')\n",
    "daily_symbols_df = all_df[all_df['symbol'].isin(experiment_symbols)]\n",
    "\n",
    "print('Records being used', len(daily_symbols_df))\n",
    "\n",
    "# Determine the min and max range for our symbols\n",
    "all_possible_dates = daily_symbols_df['quoteDate'].dropna().drop_duplicates()\n",
    "overall_min_date = all_possible_dates.min()\n",
    "overall_max_date = all_possible_dates.max()\n",
    "print('Min date for recordset', overall_min_date)\n",
    "print('Max date for recordset', overall_max_date)\n",
    "\n",
    "reference_whole_market_data = daily_symbols_df[WHOLE_MARKET_COLUMNS].drop_duplicates()\n",
    "\n",
    "# Ensure there is only one record per day\n",
    "reference_whole_market_data = reference_whole_market_data.groupby(\n",
    "    'quoteDate').first()\n",
    "reference_whole_market_data['quoteDate'] = reference_whole_market_data.index\n",
    "\n",
    "# Convert records to weekly medi    \n",
    "median_whole_market_data = return_week_median(reference_whole_market_data)\n",
    "print('Number of weeks in dataset', len(median_whole_market_data))\n",
    "\n",
    "\n",
    "# Retrieve date parameters for start and finish\n",
    "symbols_median_dfs = []\n",
    "\n",
    "# Create symbol dfs with daily data, the convert to weekly median data\n",
    "for symbol in experiment_symbols:\n",
    "    # Filter to model data for this symbol and re-set the pandas indexes\n",
    "    model_data = daily_symbols_df.loc[daily_symbols_df['symbol'] == symbol][SYMBOL_COLS]\n",
    "    \n",
    "    print('Symbol:', symbol, 'number of daily records:', len(model_data))\n",
    "        \n",
    "#     print('Converting to week median values')    \n",
    "    # Conert date to number\n",
    "    model_data['exDividendRelative'] = model_data['exDividendDate'] - model_data['quoteDate']\n",
    "    # convert string difference value to integer\n",
    "    model_data['exDividendRelative'] = model_data['exDividendRelative'].apply(\n",
    "        lambda x: np.nan if pd.isnull(x) else x.days)\n",
    "    # Make sure it is the minimum data type size\n",
    "    model_data.loc[:, 'exDividendRelative'] = model_data['exDividendRelative'].astype('int32', errors='ignore')\n",
    "        \n",
    "    median_model_data = return_week_median(model_data)\n",
    "    \n",
    "    # Re-add in key categorical values\n",
    "    median_model_data['symbol'] = symbol\n",
    "        \n",
    "    # Calculate weekly change percentage allowing for 0 values\n",
    "    median_model_data['price_change_perc'] = (median_model_data['adjustedPrice'] - \\\n",
    "                                                median_model_data['adjustedPrice'].shift(1)) / \\\n",
    "                                                median_model_data['adjustedPrice'].shift(1).clip(lower=0.1) * \\\n",
    "                                                100\n",
    "\n",
    "    print('Symbol:', symbol, 'number of median records:', len(median_model_data), 'average change:', np.mean(median_model_data['price_change_perc']))\n",
    "    print('-'*10)\n",
    "                \n",
    "    symbols_median_dfs.append(median_model_data)\n",
    "    symbol_counter += 1\n",
    "    \n",
    "\n",
    "# Create concatenated dataframe with all weekly median data\n",
    "print('Creating concatenated dataframes')\n",
    "symbols_median_df = pd.concat(symbols_median_dfs)\n",
    "\n",
    "print('Number of symbol median records', len(symbols_median_df))\n",
    "# symbols_median_df\n",
    "\n",
    "target_cols = []\n",
    "\n",
    "# Work through median values for each symbol and to the whole market data or -999 for no value\n",
    "for symbol in experiment_symbols:\n",
    "    symbol_df = symbols_median_df.loc[symbols_median_df['symbol'] == symbol]\n",
    "#     print(symbol_df.columns)\n",
    "    symbol_df.drop(['symbol'], axis=1, inplace=True)\n",
    "    symbol_df = symbol_df.add_prefix(symbol + '_')\n",
    "    \n",
    "    # Add target column name for future y data\n",
    "    target_cols.append(symbol)\n",
    "\n",
    "    median_whole_market_data = median_whole_market_data.merge(symbol_df, left_index=True, right_index=True, \n",
    "                                                              how='left', suffixes = ('wm_', '_' + symbol))\n",
    "\n",
    "#     median_whole_market_data[symbol_df.columns.values] = median_whole_market_data[symbol_df.columns.values].fillna(-999)\n",
    "    median_whole_market_data.fillna(-999, inplace=True)\n",
    "\n",
    "    \n",
    "print(median_whole_market_data.shape)\n",
    "print('Num target cols', len(target_cols))\n",
    "\n",
    "\n",
    "    \n",
    "# Create tagrte vector using the stationary vals for the following week \n",
    "whole_market_targets = median_whole_market_data[target_cols].shift(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframes\n",
    "df_train_x.to_pickle(\"../data/ml-rnn-experiment-baseline.pkl.gz\", compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing - separate symbol rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_future_cols(df):\n",
    "    cols_to_drop = []\n",
    "    for col in df.columns:\n",
    "        if 'future' in col or col.startswith('_'):\n",
    "            cols_to_drop.append(col)\n",
    "\n",
    "    return df.drop(cols_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ensemble_processing import execute_one_hot_string_encoder, train_rar_encoder\n",
    "from processing_constants import COLUMNS_TO_REMOVE\n",
    "from sklearn.preprocessing import MinMaxScaler, Imputer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "target_col = 'price_change_perc'\n",
    "\n",
    "all_train_x = df_train_x.dropna(subset=[target_col])\n",
    "all_train_x = remove_future_cols(all_train_x)\n",
    "all_train_x.drop(COLUMNS_TO_REMOVE, axis=1, errors='ignore', inplace=True)\n",
    "categorical_vals = ['symbol', 'GICSSector', 'GICSIndustryGroup', 'GICSIndustry']\n",
    "symbol_lookup = all_train_x['symbol']\n",
    "\n",
    "print('Encoding categorical vals')\n",
    "cat_enc = OrdinalEncoder()\n",
    "all_train_x[categorical_vals] = cat_enc.fit_transform(all_train_x[categorical_vals].values)\n",
    "\n",
    "all_val_cols = []\n",
    "\n",
    "for col in all_train_x.columns:\n",
    "    if col != target_col:\n",
    "        all_val_cols.append(col)\n",
    "        \n",
    "print(all_val_cols)\n",
    "\n",
    "print('Training and executing imputer')\n",
    "imputer = Imputer(strategy='median')\n",
    "all_train_x[all_val_cols] = imputer.fit_transform(all_train_x[all_val_cols].values)\n",
    "\n",
    "print('Training and executing scaler')\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "all_train_x[all_val_cols] = scaler.fit_transform(all_train_x[all_val_cols].values)\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_train_x.isnull().sum()\n",
    "# all_train_x['_13']\n",
    "# all_train_x.drop(['_13'], axis=1, inplace=True)\n",
    "# all_df['exDividendDate'].isnull()\n",
    "symbol_lookup == 'GSW'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing - flat all symbols in each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for nans\n",
      "X nans: allordpreviousclose           0\n",
      "allorddayshigh                0\n",
      "allorddayslow                 0\n",
      "asxpreviousclose              0\n",
      "asxdayshigh                   0\n",
      "asxdayslow                    0\n",
      "640106_A3597525W              0\n",
      "FIRMMCRT                      0\n",
      "FXRUSD                        0\n",
      "GRCPAIAD                      0\n",
      "GRCPAISAD                     0\n",
      "GRCPBCAD                      0\n",
      "GRCPBCSAD                     0\n",
      "GRCPBMAD                      0\n",
      "GRCPNRAD                      0\n",
      "GRCPRCAD                      0\n",
      "H01_GGDPCVGDP                 0\n",
      "H01_GGDPCVGDPFY               0\n",
      "H05_GLFSEPTPOP                0\n",
      "GSW_adjustedPrice             0\n",
      "GSW_AINTCOV                   0\n",
      "GSW_Beta                      0\n",
      "GSW_BookValuePerShareYear     0\n",
      "GSW_CashPerShareYear          0\n",
      "GSW_DPSRecentYear             0\n",
      "GSW_EPS                       0\n",
      "GSW_Float                     0\n",
      "GSW_MarketCap                 0\n",
      "GSW_OperatingMargin           0\n",
      "GSW_PE                        0\n",
      "GSW_ReturnOnEquityYear        0\n",
      "GSW_TotalDebtToEquityYear     0\n",
      "GSW_exDividendRelative        0\n",
      "GSW_price_change_perc         0\n",
      "VGAD_adjustedPrice            0\n",
      "VGAD_AINTCOV                  0\n",
      "VGAD_Beta                     0\n",
      "VGAD_BookValuePerShareYear    0\n",
      "VGAD_CashPerShareYear         0\n",
      "VGAD_DPSRecentYear            0\n",
      "VGAD_EPS                      0\n",
      "VGAD_Float                    0\n",
      "VGAD_MarketCap                0\n",
      "VGAD_OperatingMargin          0\n",
      "VGAD_PE                       0\n",
      "VGAD_ReturnOnEquityYear       0\n",
      "VGAD_TotalDebtToEquityYear    0\n",
      "VGAD_exDividendRelative       0\n",
      "VGAD_price_change_perc        0\n",
      "CUA_adjustedPrice             0\n",
      "CUA_AINTCOV                   0\n",
      "CUA_Beta                      0\n",
      "CUA_BookValuePerShareYear     0\n",
      "CUA_CashPerShareYear          0\n",
      "CUA_DPSRecentYear             0\n",
      "CUA_EPS                       0\n",
      "CUA_Float                     0\n",
      "CUA_MarketCap                 0\n",
      "CUA_OperatingMargin           0\n",
      "CUA_PE                        0\n",
      "CUA_ReturnOnEquityYear        0\n",
      "CUA_TotalDebtToEquityYear     0\n",
      "CUA_exDividendRelative        0\n",
      "CUA_price_change_perc         0\n",
      "TIX_adjustedPrice             0\n",
      "TIX_AINTCOV                   0\n",
      "TIX_Beta                      0\n",
      "TIX_BookValuePerShareYear     0\n",
      "TIX_CashPerShareYear          0\n",
      "TIX_DPSRecentYear             0\n",
      "TIX_EPS                       0\n",
      "TIX_Float                     0\n",
      "TIX_MarketCap                 0\n",
      "TIX_OperatingMargin           0\n",
      "TIX_PE                        0\n",
      "TIX_ReturnOnEquityYear        0\n",
      "TIX_TotalDebtToEquityYear     0\n",
      "TIX_exDividendRelative        0\n",
      "TIX_price_change_perc         0\n",
      "FLT_adjustedPrice             0\n",
      "FLT_AINTCOV                   0\n",
      "FLT_Beta                      0\n",
      "FLT_BookValuePerShareYear     0\n",
      "FLT_CashPerShareYear          0\n",
      "FLT_DPSRecentYear             0\n",
      "FLT_EPS                       0\n",
      "FLT_Float                     0\n",
      "FLT_MarketCap                 0\n",
      "FLT_OperatingMargin           0\n",
      "FLT_PE                        0\n",
      "FLT_ReturnOnEquityYear        0\n",
      "FLT_TotalDebtToEquityYear     0\n",
      "FLT_exDividendRelative        0\n",
      "FLT_price_change_perc         0\n",
      "PTN_adjustedPrice             0\n",
      "PTN_AINTCOV                   0\n",
      "PTN_Beta                      0\n",
      "PTN_BookValuePerShareYear     0\n",
      "PTN_CashPerShareYear          0\n",
      "PTN_DPSRecentYear             0\n",
      "PTN_EPS                       0\n",
      "PTN_Float                     0\n",
      "PTN_MarketCap                 0\n",
      "PTN_OperatingMargin           0\n",
      "PTN_PE                        0\n",
      "PTN_ReturnOnEquityYear        0\n",
      "PTN_TotalDebtToEquityYear     0\n",
      "PTN_exDividendRelative        0\n",
      "PTN_price_change_perc         0\n",
      "OVN_adjustedPrice             0\n",
      "OVN_AINTCOV                   0\n",
      "OVN_Beta                      0\n",
      "OVN_BookValuePerShareYear     0\n",
      "OVN_CashPerShareYear          0\n",
      "OVN_DPSRecentYear             0\n",
      "OVN_EPS                       0\n",
      "OVN_Float                     0\n",
      "OVN_MarketCap                 0\n",
      "OVN_OperatingMargin           0\n",
      "OVN_PE                        0\n",
      "OVN_ReturnOnEquityYear        0\n",
      "OVN_TotalDebtToEquityYear     0\n",
      "OVN_exDividendRelative        0\n",
      "OVN_price_change_perc         0\n",
      "IRU_adjustedPrice             0\n",
      "IRU_AINTCOV                   0\n",
      "IRU_Beta                      0\n",
      "IRU_BookValuePerShareYear     0\n",
      "IRU_CashPerShareYear          0\n",
      "IRU_DPSRecentYear             0\n",
      "IRU_EPS                       0\n",
      "IRU_Float                     0\n",
      "IRU_MarketCap                 0\n",
      "IRU_OperatingMargin           0\n",
      "IRU_PE                        0\n",
      "IRU_ReturnOnEquityYear        0\n",
      "IRU_TotalDebtToEquityYear     0\n",
      "IRU_exDividendRelative        0\n",
      "IRU_price_change_perc         0\n",
      "FDM_adjustedPrice             0\n",
      "FDM_AINTCOV                   0\n",
      "FDM_Beta                      0\n",
      "FDM_BookValuePerShareYear     0\n",
      "FDM_CashPerShareYear          0\n",
      "FDM_DPSRecentYear             0\n",
      "FDM_EPS                       0\n",
      "FDM_Float                     0\n",
      "FDM_MarketCap                 0\n",
      "FDM_OperatingMargin           0\n",
      "FDM_PE                        0\n",
      "FDM_ReturnOnEquityYear        0\n",
      "FDM_TotalDebtToEquityYear     0\n",
      "FDM_exDividendRelative        0\n",
      "FDM_price_change_perc         0\n",
      "HDF_adjustedPrice             0\n",
      "HDF_AINTCOV                   0\n",
      "HDF_Beta                      0\n",
      "HDF_BookValuePerShareYear     0\n",
      "HDF_CashPerShareYear          0\n",
      "HDF_DPSRecentYear             0\n",
      "HDF_EPS                       0\n",
      "HDF_Float                     0\n",
      "HDF_MarketCap                 0\n",
      "HDF_OperatingMargin           0\n",
      "HDF_PE                        0\n",
      "HDF_ReturnOnEquityYear        0\n",
      "HDF_TotalDebtToEquityYear     0\n",
      "HDF_exDividendRelative        0\n",
      "HDF_price_change_perc         0\n",
      "SVW_adjustedPrice             0\n",
      "SVW_AINTCOV                   0\n",
      "SVW_Beta                      0\n",
      "SVW_BookValuePerShareYear     0\n",
      "SVW_CashPerShareYear          0\n",
      "SVW_DPSRecentYear             0\n",
      "SVW_EPS                       0\n",
      "SVW_Float                     0\n",
      "SVW_MarketCap                 0\n",
      "SVW_OperatingMargin           0\n",
      "SVW_PE                        0\n",
      "SVW_ReturnOnEquityYear        0\n",
      "SVW_TotalDebtToEquityYear     0\n",
      "SVW_exDividendRelative        0\n",
      "SVW_price_change_perc         0\n",
      "BWX_adjustedPrice             0\n",
      "BWX_AINTCOV                   0\n",
      "BWX_Beta                      0\n",
      "BWX_BookValuePerShareYear     0\n",
      "BWX_CashPerShareYear          0\n",
      "BWX_DPSRecentYear             0\n",
      "BWX_EPS                       0\n",
      "BWX_Float                     0\n",
      "BWX_MarketCap                 0\n",
      "BWX_OperatingMargin           0\n",
      "BWX_PE                        0\n",
      "BWX_ReturnOnEquityYear        0\n",
      "BWX_TotalDebtToEquityYear     0\n",
      "BWX_exDividendRelative        0\n",
      "BWX_price_change_perc         0\n",
      "FPH_adjustedPrice             0\n",
      "FPH_AINTCOV                   0\n",
      "FPH_Beta                      0\n",
      "FPH_BookValuePerShareYear     0\n",
      "FPH_CashPerShareYear          0\n",
      "FPH_DPSRecentYear             0\n",
      "FPH_EPS                       0\n",
      "FPH_Float                     0\n",
      "FPH_MarketCap                 0\n",
      "FPH_OperatingMargin           0\n",
      "FPH_PE                        0\n",
      "FPH_ReturnOnEquityYear        0\n",
      "FPH_TotalDebtToEquityYear     0\n",
      "FPH_exDividendRelative        0\n",
      "FPH_price_change_perc         0\n",
      "AGL_adjustedPrice             0\n",
      "AGL_AINTCOV                   0\n",
      "AGL_Beta                      0\n",
      "AGL_BookValuePerShareYear     0\n",
      "AGL_CashPerShareYear          0\n",
      "AGL_DPSRecentYear             0\n",
      "AGL_EPS                       0\n",
      "AGL_Float                     0\n",
      "AGL_MarketCap                 0\n",
      "AGL_OperatingMargin           0\n",
      "AGL_PE                        0\n",
      "AGL_ReturnOnEquityYear        0\n",
      "AGL_TotalDebtToEquityYear     0\n",
      "AGL_exDividendRelative        0\n",
      "AGL_price_change_perc         0\n",
      "JHC_adjustedPrice             0\n",
      "JHC_AINTCOV                   0\n",
      "JHC_Beta                      0\n",
      "JHC_BookValuePerShareYear     0\n",
      "JHC_CashPerShareYear          0\n",
      "JHC_DPSRecentYear             0\n",
      "JHC_EPS                       0\n",
      "JHC_Float                     0\n",
      "JHC_MarketCap                 0\n",
      "JHC_OperatingMargin           0\n",
      "JHC_PE                        0\n",
      "JHC_ReturnOnEquityYear        0\n",
      "JHC_TotalDebtToEquityYear     0\n",
      "JHC_exDividendRelative        0\n",
      "JHC_price_change_perc         0\n",
      "HIN_adjustedPrice             0\n",
      "HIN_AINTCOV                   0\n",
      "HIN_Beta                      0\n",
      "HIN_BookValuePerShareYear     0\n",
      "HIN_CashPerShareYear          0\n",
      "HIN_DPSRecentYear             0\n",
      "                             ..\n",
      "ALT_DPSRecentYear             0\n",
      "ALT_EPS                       0\n",
      "ALT_Float                     0\n",
      "ALT_MarketCap                 0\n",
      "ALT_OperatingMargin           0\n",
      "ALT_PE                        0\n",
      "ALT_ReturnOnEquityYear        0\n",
      "ALT_TotalDebtToEquityYear     0\n",
      "ALT_exDividendRelative        0\n",
      "ALT_price_change_perc         0\n",
      "SES_adjustedPrice             0\n",
      "SES_AINTCOV                   0\n",
      "SES_Beta                      0\n",
      "SES_BookValuePerShareYear     0\n",
      "SES_CashPerShareYear          0\n",
      "SES_DPSRecentYear             0\n",
      "SES_EPS                       0\n",
      "SES_Float                     0\n",
      "SES_MarketCap                 0\n",
      "SES_OperatingMargin           0\n",
      "SES_PE                        0\n",
      "SES_ReturnOnEquityYear        0\n",
      "SES_TotalDebtToEquityYear     0\n",
      "SES_exDividendRelative        0\n",
      "SES_price_change_perc         0\n",
      "RNO_adjustedPrice             0\n",
      "RNO_AINTCOV                   0\n",
      "RNO_Beta                      0\n",
      "RNO_BookValuePerShareYear     0\n",
      "RNO_CashPerShareYear          0\n",
      "RNO_DPSRecentYear             0\n",
      "RNO_EPS                       0\n",
      "RNO_Float                     0\n",
      "RNO_MarketCap                 0\n",
      "RNO_OperatingMargin           0\n",
      "RNO_PE                        0\n",
      "RNO_ReturnOnEquityYear        0\n",
      "RNO_TotalDebtToEquityYear     0\n",
      "RNO_exDividendRelative        0\n",
      "RNO_price_change_perc         0\n",
      "MXC_adjustedPrice             0\n",
      "MXC_AINTCOV                   0\n",
      "MXC_Beta                      0\n",
      "MXC_BookValuePerShareYear     0\n",
      "MXC_CashPerShareYear          0\n",
      "MXC_DPSRecentYear             0\n",
      "MXC_EPS                       0\n",
      "MXC_Float                     0\n",
      "MXC_MarketCap                 0\n",
      "MXC_OperatingMargin           0\n",
      "MXC_PE                        0\n",
      "MXC_ReturnOnEquityYear        0\n",
      "MXC_TotalDebtToEquityYear     0\n",
      "MXC_exDividendRelative        0\n",
      "MXC_price_change_perc         0\n",
      "IRC_adjustedPrice             0\n",
      "IRC_AINTCOV                   0\n",
      "IRC_Beta                      0\n",
      "IRC_BookValuePerShareYear     0\n",
      "IRC_CashPerShareYear          0\n",
      "IRC_DPSRecentYear             0\n",
      "IRC_EPS                       0\n",
      "IRC_Float                     0\n",
      "IRC_MarketCap                 0\n",
      "IRC_OperatingMargin           0\n",
      "IRC_PE                        0\n",
      "IRC_ReturnOnEquityYear        0\n",
      "IRC_TotalDebtToEquityYear     0\n",
      "IRC_exDividendRelative        0\n",
      "IRC_price_change_perc         0\n",
      "GTR_adjustedPrice             0\n",
      "GTR_AINTCOV                   0\n",
      "GTR_Beta                      0\n",
      "GTR_BookValuePerShareYear     0\n",
      "GTR_CashPerShareYear          0\n",
      "GTR_DPSRecentYear             0\n",
      "GTR_EPS                       0\n",
      "GTR_Float                     0\n",
      "GTR_MarketCap                 0\n",
      "GTR_OperatingMargin           0\n",
      "GTR_PE                        0\n",
      "GTR_ReturnOnEquityYear        0\n",
      "GTR_TotalDebtToEquityYear     0\n",
      "GTR_exDividendRelative        0\n",
      "GTR_price_change_perc         0\n",
      "AYM_adjustedPrice             0\n",
      "AYM_AINTCOV                   0\n",
      "AYM_Beta                      0\n",
      "AYM_BookValuePerShareYear     0\n",
      "AYM_CashPerShareYear          0\n",
      "AYM_DPSRecentYear             0\n",
      "AYM_EPS                       0\n",
      "AYM_Float                     0\n",
      "AYM_MarketCap                 0\n",
      "AYM_OperatingMargin           0\n",
      "AYM_PE                        0\n",
      "AYM_ReturnOnEquityYear        0\n",
      "AYM_TotalDebtToEquityYear     0\n",
      "AYM_exDividendRelative        0\n",
      "AYM_price_change_perc         0\n",
      "RLC_adjustedPrice             0\n",
      "RLC_AINTCOV                   0\n",
      "RLC_Beta                      0\n",
      "RLC_BookValuePerShareYear     0\n",
      "RLC_CashPerShareYear          0\n",
      "RLC_DPSRecentYear             0\n",
      "RLC_EPS                       0\n",
      "RLC_Float                     0\n",
      "RLC_MarketCap                 0\n",
      "RLC_OperatingMargin           0\n",
      "RLC_PE                        0\n",
      "RLC_ReturnOnEquityYear        0\n",
      "RLC_TotalDebtToEquityYear     0\n",
      "RLC_exDividendRelative        0\n",
      "RLC_price_change_perc         0\n",
      "IVG_adjustedPrice             0\n",
      "IVG_AINTCOV                   0\n",
      "IVG_Beta                      0\n",
      "IVG_BookValuePerShareYear     0\n",
      "IVG_CashPerShareYear          0\n",
      "IVG_DPSRecentYear             0\n",
      "IVG_EPS                       0\n",
      "IVG_Float                     0\n",
      "IVG_MarketCap                 0\n",
      "IVG_OperatingMargin           0\n",
      "IVG_PE                        0\n",
      "IVG_ReturnOnEquityYear        0\n",
      "IVG_TotalDebtToEquityYear     0\n",
      "IVG_exDividendRelative        0\n",
      "IVG_price_change_perc         0\n",
      "MDG_adjustedPrice             0\n",
      "MDG_AINTCOV                   0\n",
      "MDG_Beta                      0\n",
      "MDG_BookValuePerShareYear     0\n",
      "MDG_CashPerShareYear          0\n",
      "MDG_DPSRecentYear             0\n",
      "MDG_EPS                       0\n",
      "MDG_Float                     0\n",
      "MDG_MarketCap                 0\n",
      "MDG_OperatingMargin           0\n",
      "MDG_PE                        0\n",
      "MDG_ReturnOnEquityYear        0\n",
      "MDG_TotalDebtToEquityYear     0\n",
      "MDG_exDividendRelative        0\n",
      "MDG_price_change_perc         0\n",
      "MPE_adjustedPrice             0\n",
      "MPE_AINTCOV                   0\n",
      "MPE_Beta                      0\n",
      "MPE_BookValuePerShareYear     0\n",
      "MPE_CashPerShareYear          0\n",
      "MPE_DPSRecentYear             0\n",
      "MPE_EPS                       0\n",
      "MPE_Float                     0\n",
      "MPE_MarketCap                 0\n",
      "MPE_OperatingMargin           0\n",
      "MPE_PE                        0\n",
      "MPE_ReturnOnEquityYear        0\n",
      "MPE_TotalDebtToEquityYear     0\n",
      "MPE_exDividendRelative        0\n",
      "MPE_price_change_perc         0\n",
      "MOT_adjustedPrice             0\n",
      "MOT_AINTCOV                   0\n",
      "MOT_Beta                      0\n",
      "MOT_BookValuePerShareYear     0\n",
      "MOT_CashPerShareYear          0\n",
      "MOT_DPSRecentYear             0\n",
      "MOT_EPS                       0\n",
      "MOT_Float                     0\n",
      "MOT_MarketCap                 0\n",
      "MOT_OperatingMargin           0\n",
      "MOT_PE                        0\n",
      "MOT_ReturnOnEquityYear        0\n",
      "MOT_TotalDebtToEquityYear     0\n",
      "MOT_exDividendRelative        0\n",
      "MOT_price_change_perc         0\n",
      "OAR_adjustedPrice             0\n",
      "OAR_AINTCOV                   0\n",
      "OAR_Beta                      0\n",
      "OAR_BookValuePerShareYear     0\n",
      "OAR_CashPerShareYear          0\n",
      "OAR_DPSRecentYear             0\n",
      "OAR_EPS                       0\n",
      "OAR_Float                     0\n",
      "OAR_MarketCap                 0\n",
      "OAR_OperatingMargin           0\n",
      "OAR_PE                        0\n",
      "OAR_ReturnOnEquityYear        0\n",
      "OAR_TotalDebtToEquityYear     0\n",
      "OAR_exDividendRelative        0\n",
      "OAR_price_change_perc         0\n",
      "HGL_adjustedPrice             0\n",
      "HGL_AINTCOV                   0\n",
      "HGL_Beta                      0\n",
      "HGL_BookValuePerShareYear     0\n",
      "HGL_CashPerShareYear          0\n",
      "HGL_DPSRecentYear             0\n",
      "HGL_EPS                       0\n",
      "HGL_Float                     0\n",
      "HGL_MarketCap                 0\n",
      "HGL_OperatingMargin           0\n",
      "HGL_PE                        0\n",
      "HGL_ReturnOnEquityYear        0\n",
      "HGL_TotalDebtToEquityYear     0\n",
      "HGL_exDividendRelative        0\n",
      "HGL_price_change_perc         0\n",
      "DSE_adjustedPrice             0\n",
      "DSE_AINTCOV                   0\n",
      "DSE_Beta                      0\n",
      "DSE_BookValuePerShareYear     0\n",
      "DSE_CashPerShareYear          0\n",
      "DSE_DPSRecentYear             0\n",
      "DSE_EPS                       0\n",
      "DSE_Float                     0\n",
      "DSE_MarketCap                 0\n",
      "DSE_OperatingMargin           0\n",
      "DSE_PE                        0\n",
      "DSE_ReturnOnEquityYear        0\n",
      "DSE_TotalDebtToEquityYear     0\n",
      "DSE_exDividendRelative        0\n",
      "DSE_price_change_perc         0\n",
      "VII_adjustedPrice             0\n",
      "VII_AINTCOV                   0\n",
      "VII_Beta                      0\n",
      "VII_BookValuePerShareYear     0\n",
      "VII_CashPerShareYear          0\n",
      "VII_DPSRecentYear             0\n",
      "VII_EPS                       0\n",
      "VII_Float                     0\n",
      "VII_MarketCap                 0\n",
      "VII_OperatingMargin           0\n",
      "VII_PE                        0\n",
      "VII_ReturnOnEquityYear        0\n",
      "VII_TotalDebtToEquityYear     0\n",
      "VII_exDividendRelative        0\n",
      "VII_price_change_perc         0\n",
      "KTL_adjustedPrice             0\n",
      "KTL_AINTCOV                   0\n",
      "KTL_Beta                      0\n",
      "KTL_BookValuePerShareYear     0\n",
      "KTL_CashPerShareYear          0\n",
      "KTL_DPSRecentYear             0\n",
      "KTL_EPS                       0\n",
      "KTL_Float                     0\n",
      "KTL_MarketCap                 0\n",
      "KTL_OperatingMargin           0\n",
      "KTL_PE                        0\n",
      "KTL_ReturnOnEquityYear        0\n",
      "KTL_TotalDebtToEquityYear     0\n",
      "KTL_exDividendRelative        0\n",
      "KTL_price_change_perc         0\n",
      "Length: 1714, dtype: int64 y nans: GSW_price_change_perc     1\n",
      "VGAD_price_change_perc    1\n",
      "CUA_price_change_perc     1\n",
      "TIX_price_change_perc     1\n",
      "FLT_price_change_perc     1\n",
      "PTN_price_change_perc     1\n",
      "OVN_price_change_perc     1\n",
      "IRU_price_change_perc     1\n",
      "FDM_price_change_perc     1\n",
      "HDF_price_change_perc     1\n",
      "SVW_price_change_perc     1\n",
      "BWX_price_change_perc     1\n",
      "FPH_price_change_perc     1\n",
      "AGL_price_change_perc     1\n",
      "JHC_price_change_perc     1\n",
      "HIN_price_change_perc     1\n",
      "TRS_price_change_perc     1\n",
      "HGO_price_change_perc     1\n",
      "SGP_price_change_perc     1\n",
      "AVN_price_change_perc     1\n",
      "DOW_price_change_perc     1\n",
      "NVL_price_change_perc     1\n",
      "AOG_price_change_perc     1\n",
      "BGL_price_change_perc     1\n",
      "HSN_price_change_perc     1\n",
      "CSS_price_change_perc     1\n",
      "SWM_price_change_perc     1\n",
      "RIC_price_change_perc     1\n",
      "CMI_price_change_perc     1\n",
      "MKE_price_change_perc     1\n",
      "BCI_price_change_perc     1\n",
      "IBG_price_change_perc     1\n",
      "SWJ_price_change_perc     1\n",
      "IXP_price_change_perc     1\n",
      "MYX_price_change_perc     1\n",
      "RNY_price_change_perc     1\n",
      "CZA_price_change_perc     1\n",
      "SRV_price_change_perc     1\n",
      "VMT_price_change_perc     1\n",
      "CDM_price_change_perc     1\n",
      "CLQ_price_change_perc     1\n",
      "BPS_price_change_perc     1\n",
      "CSE_price_change_perc     1\n",
      "ORR_price_change_perc     1\n",
      "OOK_price_change_perc     1\n",
      "EOS_price_change_perc     1\n",
      "BRL_price_change_perc     1\n",
      "BWP_price_change_perc     1\n",
      "ENC_price_change_perc     1\n",
      "FCT_price_change_perc     1\n",
      "SIO_price_change_perc     1\n",
      "SZG_price_change_perc     1\n",
      "ECG_price_change_perc     1\n",
      "SXA_price_change_perc     1\n",
      "MHC_price_change_perc     1\n",
      "PHK_price_change_perc     1\n",
      "ICT_price_change_perc     1\n",
      "CLH_price_change_perc     1\n",
      "NTM_price_change_perc     1\n",
      "TAS_price_change_perc     1\n",
      "REV_price_change_perc     1\n",
      "CII_price_change_perc     1\n",
      "FND_price_change_perc     1\n",
      "ZGM_price_change_perc     1\n",
      "GOE_price_change_perc     1\n",
      "SSI_price_change_perc     1\n",
      "QMN_price_change_perc     1\n",
      "EAL_price_change_perc     1\n",
      "DSB_price_change_perc     1\n",
      "PNX_price_change_perc     1\n",
      "KRS_price_change_perc     1\n",
      "RXH_price_change_perc     1\n",
      "LAU_price_change_perc     1\n",
      "CNW_price_change_perc     1\n",
      "GSZ_price_change_perc     1\n",
      "IBC_price_change_perc     1\n",
      "SAS_price_change_perc     1\n",
      "EAS_price_change_perc     1\n",
      "OCC_price_change_perc     1\n",
      "AIS_price_change_perc     1\n",
      "INK_price_change_perc     1\n",
      "AIK_price_change_perc     1\n",
      "EVM_price_change_perc     1\n",
      "MSV_price_change_perc     1\n",
      "XST_price_change_perc     1\n",
      "GMR_price_change_perc     1\n",
      "WWI_price_change_perc     1\n",
      "JYC_price_change_perc     1\n",
      "BSM_price_change_perc     1\n",
      "VRX_price_change_perc     1\n",
      "TKL_price_change_perc     1\n",
      "WFE_price_change_perc     1\n",
      "ZMI_price_change_perc     1\n",
      "SHK_price_change_perc     1\n",
      "DGO_price_change_perc     1\n",
      "BD1_price_change_perc     1\n",
      "ALT_price_change_perc     1\n",
      "SES_price_change_perc     1\n",
      "RNO_price_change_perc     1\n",
      "MXC_price_change_perc     1\n",
      "IRC_price_change_perc     1\n",
      "GTR_price_change_perc     1\n",
      "AYM_price_change_perc     1\n",
      "RLC_price_change_perc     1\n",
      "IVG_price_change_perc     1\n",
      "MDG_price_change_perc     1\n",
      "MPE_price_change_perc     1\n",
      "MOT_price_change_perc     1\n",
      "OAR_price_change_perc     1\n",
      "HGL_price_change_perc     1\n",
      "DSE_price_change_perc     1\n",
      "VII_price_change_perc     1\n",
      "KTL_price_change_perc     1\n",
      "dtype: int64\n",
      "Total y nans: 113\n",
      "Expected y nans: 113\n",
      "Training and executing scaler\n",
      "Scaling done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "print('Checking for nans')\n",
    "x_msk = median_whole_market_data.isnull()\n",
    "y_msk = whole_market_targets.isnull()\n",
    "\n",
    "print('X nans:', x_msk.sum(), 'y nans:', y_msk.sum())\n",
    "\n",
    "total_ys = np.sum(y_msk.sum())\n",
    "print('Total y nans:', total_ys)\n",
    "print('Expected y nans:', len(target_cols))\n",
    "\n",
    "if total_ys != len(target_cols):\n",
    "    print('WARNING - UNEXPECTED NaNs')\n",
    "\n",
    "\n",
    "print('Training and executing scaler')\n",
    "x_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "all_x = x_scaler.fit_transform(median_whole_market_data[:-1].values)\n",
    "\n",
    "\n",
    "# y_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# all_train_y = y_scaler.fit_transform(whole_market_targets[:-1].values)\n",
    "\n",
    "all_y = whole_market_targets[:-1].values\n",
    "\n",
    "print('Scaling done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to X and Y sequences - flat all symbols in each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "602 y vals, 14310 padding vals (-999)\n",
      "Updating -999 to 0\n",
      "602 y vals, 0 padding vals (-999)\n",
      "--------------------\n",
      "Overall totals\n",
      " - train timesteps 16\n",
      " - train features 1714\n",
      " - train outputs 8\n",
      " - train X shape:  (468, 16, 1714)\n",
      " - train y shape:  (468, 8, 113)\n",
      " - test timesteps 16\n",
      " - test features 1714\n",
      " - test outputs 8\n",
      " - test X shape:  (110, 16, 1714)\n",
      " - test y shape:  (110, 8, 113)\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "\n",
    "# input and output time steps \n",
    "n_input = 16\n",
    "n_out = 8\n",
    "\n",
    "rnn_flat_x = list()\n",
    "rnn_flat_y = list()\n",
    "\n",
    "rnn_flat_test_x = list()\n",
    "rnn_flat_test_y = list()\n",
    "\n",
    "# number of features\n",
    "n_features = all_x.shape[1]\n",
    "#     print(n_features)\n",
    "\n",
    "in_start = 0\n",
    "eighty_num = int((len(all_x) - n_input) * .80)\n",
    "\n",
    "# Reset y values of -999 to 0 \n",
    "padding_msk = (all_y == -999)\n",
    "print(len(all_y), 'y vals,', padding_msk.sum(), 'padding vals (-999)')\n",
    "print('Updating -999 to 0')\n",
    "all_y[padding_msk] = 0\n",
    "padding_msk = (all_y == -999)\n",
    "print(len(all_y), 'y vals,', padding_msk.sum(), 'padding vals (-999)')\n",
    "\n",
    "# step over the first 80% of the history one time step at a time - training\n",
    "for _ in range(eighty_num):\n",
    "    # define the end of the input sequence\n",
    "    in_end = in_start + n_input\n",
    "    out_end = in_end + n_out\n",
    "\n",
    "    # ensure we have enough data for this instance\n",
    "    if out_end < len(all_x):\n",
    "        rnn_flat_x.append(all_x[in_start:in_end])\n",
    "        rnn_flat_y.append(all_y[in_end:out_end])\n",
    "\n",
    "    # move along one time step\n",
    "    in_start += 1\n",
    "\n",
    "# step over the last 20% of the history one time step at a time - test\n",
    "for _ in range(eighty_num + 1, len(all_x)):\n",
    "    # define the end of the input sequence\n",
    "    in_end = in_start + n_input\n",
    "    out_end = in_end + n_out\n",
    "\n",
    "    # ensure we have enough data for this instance\n",
    "    if out_end < len(all_x):\n",
    "        rnn_flat_test_x.append(all_x[in_start:in_end])\n",
    "        rnn_flat_test_y.append(all_y[in_end:out_end])\n",
    "\n",
    "    # move along one time step\n",
    "    in_start += 1\n",
    "        \n",
    "\n",
    "rnn_flat_train_x = array(rnn_flat_x)\n",
    "rnn_flat_train_y = array(rnn_flat_y)\n",
    "\n",
    "rnn_flat_test_x = array(rnn_flat_test_x)\n",
    "rnn_flat_test_y = array(rnn_flat_test_y)\n",
    "\n",
    "n_timesteps, n_features, n_outputs, out_features = rnn_flat_train_x.shape[1], \\\n",
    "    rnn_flat_train_x.shape[2], rnn_flat_train_y.shape[1], rnn_flat_train_y.shape[2]\n",
    "\n",
    "# reshape output into [samples, timesteps, features]\n",
    "# rnn_train_y = rnn_train_y.reshape((rnn_train_y.shape[0], rnn_train_y.shape[1], rnn_train_y.shape[2]))\n",
    "\n",
    "test_n_timesteps, test_n_features, test_n_outputs = rnn_flat_test_x.shape[1], \\\n",
    "    rnn_flat_test_x.shape[2], rnn_flat_test_y.shape[1]\n",
    "\n",
    "# reshape output into [samples, timesteps, features]\n",
    "# rnn_test_y = rnn_test_y.reshape((rnn_test_y.shape[0], rnn_test_y.shape[1], 1))\n",
    "\n",
    "\n",
    "print('-'*20)\n",
    "print('Overall totals')\n",
    "\n",
    "print(' - train timesteps', n_timesteps)\n",
    "print(' - train features', n_features)\n",
    "print(' - train outputs', n_outputs)\n",
    "\n",
    "print(' - train X shape: ', rnn_flat_train_x.shape)\n",
    "print(' - train y shape: ', rnn_flat_train_y.shape)\n",
    "\n",
    "print(' - test timesteps', test_n_timesteps)\n",
    "print(' - test features', test_n_features)\n",
    "print(' - test outputs', test_n_outputs)\n",
    "\n",
    "print(' - test X shape: ', rnn_flat_test_x.shape)\n",
    "print(' - test y shape: ', rnn_flat_test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to X and Y sequences - separate symbol rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "\n",
    "# input and output time steps \n",
    "n_input = 16\n",
    "n_out = 16\n",
    "\n",
    "rnn_list_x = list()\n",
    "rnn_list_y = list()\n",
    "rnn_list_symbols = list()\n",
    "\n",
    "rnn_list_test_x = list()\n",
    "rnn_list_test_y = list()\n",
    "rnn_list_test_symbols = list()\n",
    "\n",
    "\n",
    "for symbol in experiment_symbols:\n",
    "    # Filter to model data for this symbol and re-set the pandas indexes\n",
    "    model_data = all_train_x.loc[symbol_lookup == symbol]\n",
    "    \n",
    "    X = list()\n",
    "    y = list()\n",
    "    test_X = list()\n",
    "    test_y = list()\n",
    "        \n",
    "    # nuber of features\n",
    "    n_features = model_data.shape[1]\n",
    "#     print(n_features)\n",
    "    \n",
    "    in_start = 0\n",
    "    eighty_num = int((len(model_data) - n_input) * .80)\n",
    "    \n",
    "    # step over the first 80% of the history one time step at a time - training\n",
    "    for _ in range(eighty_num):\n",
    "        # define the end of the input sequence\n",
    "        in_end = in_start + n_input\n",
    "        out_end = in_end + n_out\n",
    "        \n",
    "        # ensure we have enough data for this instance\n",
    "        if out_end < len(model_data):\n",
    "            X.append(model_data.iloc[in_start:in_end, :].values)\n",
    "            y.append(model_data.iloc[in_end:out_end, n_features-1].values)\n",
    "            \n",
    "            rnn_list_x.append(model_data.iloc[in_start:in_end, :].values)\n",
    "            rnn_list_y.append(model_data.iloc[in_end:out_end, n_features-1].values)\n",
    "            rnn_list_symbols.append(symbol)\n",
    "            \n",
    "#             if (_ % 200 == 199):\n",
    "#                 print('-'*80)\n",
    "#                 print(symbol)\n",
    "#                 print(model_data.iloc[in_start:in_end, :])\n",
    "#                 print('-'*50)\n",
    "#                 print(model_data.iloc[in_end:out_end, n_features-1])\n",
    "\n",
    "        # move along one time step\n",
    "        in_start += 1\n",
    "\n",
    "    # step over the last 20% of the history one time step at a time - test\n",
    "    for _ in range(eighty_num + 1, len(model_data)):\n",
    "        # define the end of the input sequence\n",
    "        in_end = in_start + n_input\n",
    "        out_end = in_end + n_out\n",
    "        \n",
    "        # ensure we have enough data for this instance\n",
    "        if out_end < len(model_data):\n",
    "            test_X.append(model_data.iloc[in_start:in_end, :].values)\n",
    "            test_y.append(model_data.iloc[in_end:out_end, n_features-1].values)\n",
    "            \n",
    "            rnn_list_test_x.append(model_data.iloc[in_start:in_end, :].values)\n",
    "            rnn_list_test_y.append(model_data.iloc[in_end:out_end, n_features-1].values)\n",
    "            rnn_list_test_symbols.append(symbol)\n",
    "            \n",
    "#             if (_ % 200 == 199):\n",
    "#                 print('-'*80)\n",
    "#                 print(symbol)\n",
    "#                 print(model_data.iloc[in_start:in_end, :])\n",
    "#                 print('-'*50)\n",
    "#                 print(model_data.iloc[in_end:out_end, n_features-1])\n",
    "\n",
    "        # move along one time step\n",
    "        in_start += 1\n",
    "        \n",
    "    train_x = array(X)\n",
    "    train_y = array(y)\n",
    "    test_x = array(test_X)\n",
    "    test_y = array(test_y)\n",
    "    \n",
    "    print('-' * 80)\n",
    "    print('Symbol:', symbol)\n",
    "    print(' - train sequence shape:', train_x.shape)\n",
    "    print(' - test sequence shape:', test_x.shape)\n",
    "    \n",
    "    n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n",
    "    \n",
    "    # reshape output into [samples, timesteps, features]\n",
    "    train_y = train_y.reshape((train_y.shape[0], train_y.shape[1], 1))\n",
    "\n",
    "    print('train n_timesteps', n_timesteps)\n",
    "    print('train n_features', n_features)\n",
    "    print('train n_outputs', n_outputs)\n",
    "    print('-' * 5)\n",
    "\n",
    "    test_n_timesteps, test_n_features, test_n_outputs = test_x.shape[1], test_x.shape[2], test_y.shape[1]\n",
    "    \n",
    "    # reshape output into [samples, timesteps, features]\n",
    "    test_y = test_y.reshape((test_y.shape[0], test_y.shape[1], 1))\n",
    "\n",
    "    print('test n_timesteps', test_n_timesteps)\n",
    "    print('test n_features', test_n_features)\n",
    "    print('test n_outputs', test_n_outputs)\n",
    "    print('-' * 5)\n",
    "    \n",
    "    print('train x rnn len:', len(rnn_list_x))\n",
    "    print('train y rnn len:', len(rnn_list_y))\n",
    "    print('test x rnn len:', len(rnn_list_test_x))\n",
    "    print('test y rnn len:', len(rnn_list_test_y))\n",
    "\n",
    "    \n",
    "#     print(train_x)\n",
    "#     print(train_y)\n",
    "\n",
    "# # Create concatenated dataframe with all data\n",
    "# print('Creating concatenated dataframes')\n",
    "# df_train_x = pd.concat(train_x_dfs)\n",
    "\n",
    "rnn_train_x = array(rnn_list_x)\n",
    "rnn_train_y = array(rnn_list_y)\n",
    "rnn_symbols = array(rnn_list_symbols)\n",
    "\n",
    "rnn_test_x = array(rnn_list_test_x)\n",
    "rnn_test_y = array(rnn_list_test_y)\n",
    "rnn_test_symbols = array(rnn_list_test_symbols)\n",
    "\n",
    "\n",
    "n_timesteps, n_features, n_outputs = rnn_train_x.shape[1], rnn_train_x.shape[2], rnn_train_y.shape[1]\n",
    "\n",
    "# reshape output into [samples, timesteps, features]\n",
    "rnn_train_y = rnn_train_y.reshape((rnn_train_y.shape[0], rnn_train_y.shape[1], 1))\n",
    "\n",
    "test_n_timesteps, test_n_features, test_n_outputs = rnn_test_x.shape[1], rnn_test_x.shape[2], rnn_test_y.shape[1]\n",
    "\n",
    "# reshape output into [samples, timesteps, features]\n",
    "rnn_test_y = rnn_test_y.reshape((rnn_test_y.shape[0], rnn_test_y.shape[1], 1))\n",
    "\n",
    "\n",
    "print('-'*20)\n",
    "print('Overall totals')\n",
    "\n",
    "print(' - train timesteps', n_timesteps)\n",
    "print(' - train features', n_features)\n",
    "print(' - train outputs', n_outputs)\n",
    "\n",
    "print(' - train X shape: ', rnn_train_x.shape)\n",
    "print(' - train y shape: ', rnn_train_y.shape)\n",
    "print(' - train symbols lookup shape: ', rnn_symbols.shape)\n",
    "\n",
    "print(' - test timesteps', test_n_timesteps)\n",
    "print(' - test features', test_n_features)\n",
    "print(' - test outputs', test_n_outputs)\n",
    "\n",
    "print(' - test X shape: ', rnn_test_x.shape)\n",
    "print(' - test y shape: ', rnn_test_y.shape)\n",
    "print(' - test symbols lookup shape: ', rnn_test_symbols.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_train_x.shape\n",
    "# print(df_train_x.drop(['future_eight_week_return'], axis=1, errors='ignore').columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_train_y.reshape(len(rnn_train_y),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rnn_train_x[0, :, 247])\n",
    "print(rnn_train_x[1, :, 247])\n",
    "print(rnn_train_x[2, :, 247])\n",
    "print(rnn_train_x[3, :, 247])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rnn_train_y[0])\n",
    "print(rnn_train_y[1])\n",
    "print(rnn_train_y[2])\n",
    "print(rnn_train_y[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(rnn_symbols))\n",
    "msk = rnn_symbols=='ALT'\n",
    "print(msk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction(x, y_true, y_pred):\n",
    "    \"\"\"Plots the predictions.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    x: Input sequence of shape (input_sequence_length,\n",
    "        dimension_of_signal)\n",
    "    y_true: True output sequence of shape (input_sequence_length,\n",
    "        dimension_of_signal)\n",
    "    y_pred: Predicted output sequence (input_sequence_length,\n",
    "        dimension_of_signal)\n",
    "    \"\"\"\n",
    "\n",
    "    plt.figure(figsize=(12, 3))\n",
    "\n",
    "    output_dim = x.shape[-1]\n",
    "    for j in range(output_dim):\n",
    "        past = x[:, j] \n",
    "        true = y_true[:, j]\n",
    "        pred = y_pred[:, j]\n",
    "\n",
    "        label1 = \"Seen (past) values\" if j==0 else \"_nolegend_\"\n",
    "        label2 = \"True future values\" if j==0 else \"_nolegend_\"\n",
    "        label3 = \"Predictions\" if j==0 else \"_nolegend_\"\n",
    "\n",
    "        plt.plot(range(len(past)), past, \"o--b\",\n",
    "                 label=label1)\n",
    "        plt.plot(range(len(past),\n",
    "                 len(true)+len(past)), true, \"x--b\", label=label2)\n",
    "        plt.plot(range(len(past), len(pred)+len(past)), pred, \"o--y\",\n",
    "                 label=label3)\n",
    "    plt.legend(loc='best')\n",
    "    plt.title(\"Predictions v.s. true values\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reference data\n",
    "from scipy import stats\n",
    "\n",
    "print('Min:', np.min(rnn_train_y))\n",
    "print('Max:', np.max(rnn_train_y))\n",
    "print('Median:', np.median(rnn_train_y))\n",
    "print('Modes:', stats.mode(rnn_train_y))\n",
    "print('Averages:', np.mean(rnn_train_y))\n",
    "print('25th percentiles:', np.percentile(rnn_train_y, 25))\n",
    "print('80th percentiles:', np.percentile(rnn_train_y, 80))\n",
    "print('90th percentile:', np.percentile(rnn_train_y, 90))\n",
    "\n",
    "\n",
    "for symbol in np.unique(rnn_symbols):\n",
    "    print('-'*80)\n",
    "    print(symbol)\n",
    "    print('-'*80)\n",
    "    msk = (rnn_symbols=='ALT')\n",
    "    print('Min:', np.min(rnn_train_y[msk]))\n",
    "    print('Max:', np.max(rnn_train_y[msk]))\n",
    "    print('Median:', np.median(rnn_train_y[msk]))\n",
    "    print('Modes:', stats.mode(rnn_train_y[msk]))\n",
    "    print('Averages:', np.mean(rnn_train_y[msk]))\n",
    "    print('25th percentiles:', np.percentile(rnn_train_y[msk], 25))\n",
    "    print('80th percentiles:', np.percentile(rnn_train_y[msk], 80))\n",
    "    print('90th percentile:', np.percentile(rnn_train_y[msk], 90))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, explained_variance_score, r2_score, median_absolute_error\n",
    "from stats_operations import *\n",
    "\n",
    "\n",
    "def eval_results(actual_y, predicted_y):\n",
    "    num_vals = len(actual_y)\n",
    "    # Work through each val in y\n",
    "    mae = mean_absolute_error(actual_y, predicted_y)\n",
    "    mape = safe_mape(actual_y, predicted_y)\n",
    "    cmape = clipped_mape(actual_y, predicted_y)\n",
    "    smape = symmetrical_mape(actual_y, predicted_y)\n",
    "    ae_p25 = abs_err_p(actual_y, predicted_y, 25)\n",
    "    ae_p50 = abs_err_p(actual_y, predicted_y, 50)\n",
    "    ae_p75 = abs_err_p(actual_y, predicted_y, 75)\n",
    "    ae_p90 = abs_err_p(actual_y, predicted_y, 90)\n",
    "    ape_p75 = safe_mape_p(actual_y, predicted_y, 75)\n",
    "    ape_p90 = safe_mape_p(actual_y, predicted_y, 90)    \n",
    "    rsquared = r2_score(actual_y, predicted_y)\n",
    "    explain_variance = explained_variance_score(actual_y, predicted_y)\n",
    "\n",
    "    return mae, mape, cmape, smape, ae_p25, ae_p50, ae_p75, ae_p90, rsquared, explain_variance, \\\n",
    "            ape_p75, ape_p90, num_vals\n",
    "\n",
    "def perform_eval(name, actuals, predictions):\n",
    "    eval_dict = {\n",
    "        'name': [name],\n",
    "        'num_vals': [len(actuals)]\n",
    "    }\n",
    "    \n",
    "    maes = []\n",
    "    mapes = []\n",
    "    cmapes = []\n",
    "    smapes = []\n",
    "    ae_p25s = []\n",
    "    ae_p50s = []\n",
    "    ae_p75s = []\n",
    "    ae_p90s = []\n",
    "    rsquareds = []\n",
    "    explain_variances = []\n",
    "\n",
    "    # execute eval per column\n",
    "    for _ in range(actuals.shape[1]):\n",
    "        col_actual_y = actuals[:, _]\n",
    "        col_predict_y = predictions[:, _]\n",
    "       \n",
    "        mae, mape, cmape, smape, ae_p25, ae_p50, ae_p75, ae_p90, rsquared, \\\n",
    "        explain_variance, num_vals = eval_results(col_actual_y, col_predict_y)\n",
    "\n",
    "        mean_actual_val = np.mean(col_actual_y)\n",
    "        median_actual_val = np.median(col_actual_y)\n",
    "\n",
    "        mean_predicted_val = np.mean(col_predict_y)\n",
    "        median_predicted_val = np.median(col_predict_y)\n",
    "\n",
    "        eval_dict['y_' + str(_) + '_mean_actual_val'] = [mean_actual_val]\n",
    "        eval_dict['y_' + str(_) + '_median_actual_val'] = [median_actual_val]\n",
    "        eval_dict['y_' + str(_) + '_mean_predicted_val'] = [mean_predicted_val]\n",
    "        eval_dict['y_' + str(_) + '_median_predicted_val'] = [median_predicted_val]\n",
    "        eval_dict['y_' + str(_) + '_mae'] = [mae] \n",
    "        eval_dict['y_' + str(_) + '_mape'] = [mape]\n",
    "        eval_dict['y_' + str(_) + '_cmape'] = [cmape]\n",
    "        eval_dict['y_' + str(_) + '_smape'] = [smape]\n",
    "        eval_dict['y_' + str(_) + '_abs_err_p25'] = [ae_p25]\n",
    "        eval_dict['y_' + str(_) + '_abs_err_p50'] = [ae_p50]\n",
    "        eval_dict['y_' + str(_) + '_abs_err_p75'] = [ae_p75]\n",
    "        eval_dict['y_' + str(_) + '_abs_err_p90'] = [ae_p90]\n",
    "        eval_dict['y_' + str(_) + '_rsquared'] = [rsquared]\n",
    "        eval_dict['y_' + str(_) + '_explain_variance'] = [explain_variance]\n",
    "        \n",
    "        maes.append(mae)\n",
    "        mapes.append(mape)\n",
    "        cmapes.append(cmape)\n",
    "        smapes.append(smape)\n",
    "        ae_p25s.append(ae_p25)\n",
    "        ae_p50s.append(ae_p50)\n",
    "        ae_p75s.append(ae_p75)\n",
    "        ae_p90s.append(ae_p90)\n",
    "        rsquareds.append(rsquared)\n",
    "        explain_variances.append(explain_variance)\n",
    "        \n",
    "    eval_dict['all_mae'] = np.mean(maes)\n",
    "    eval_dict['all_mape'] = np.mean(mapes)\n",
    "    eval_dict['all_cmape'] = np.mean(cmapes)\n",
    "    eval_dict['all_smape'] = np.mean(smapes)\n",
    "    eval_dict['all_abs_err_p25'] = np.mean(ae_p25s)\n",
    "    eval_dict['all_abs_err_p50'] = np.mean(ae_p50s)\n",
    "    eval_dict['all_abs_err_p75'] = np.mean(ae_p75s)\n",
    "    eval_dict['all_abs_err_p90'] = np.mean(ae_p90s)\n",
    "    eval_dict['all_rsquared'] = np.mean(rsquareds)\n",
    "    eval_dict['all_explain_variance'] = np.mean(explain_variances)\n",
    "        \n",
    "#     print(eval_dict)\n",
    "    eval_df = pd.DataFrame.from_dict(eval_dict)\n",
    "    \n",
    "    return eval_df\n",
    "\n",
    "def evaluate_results(symbols_x, predictions, actuals):\n",
    "    # Determine unique list of symbols\n",
    "    symbols = np.unique(symbols_x)\n",
    "\n",
    "    print('Overall results, number of items in prediction data:', len(predictions))\n",
    "    df_results = perform_eval('overall', actuals, predictions)\n",
    "    \n",
    "    print('Executing symbol results, number of symbols in prediction data:', len(symbols))\n",
    "\n",
    "    for symbol in symbols:\n",
    "        # Retrieve data indices which match symbols\n",
    "        pred_index = np.where(symbols_x == symbol)[0]\n",
    "\n",
    "        # Retrieve data which matches symbol\n",
    "        symbol_predictions = predictions[pred_index]\n",
    "        symbol_actuals = actuals[pred_index]\n",
    "        \n",
    "        print(symbol, 'results, number of items in prediction data:', len(symbol_predictions))\n",
    "\n",
    "        df_symbol_results = perform_eval(symbol, symbol_actuals, symbol_predictions)\n",
    "\n",
    "        # Add data frame into all results\n",
    "        df_results = pd.concat([df_results, df_symbol_results])\n",
    "\n",
    "    # output overall summary\n",
    "    means = ['all_mae', 'all_mape', 'all_cmape', 'all_smape', 'all_abs_err_p25', \n",
    "             'all_abs_err_p50', 'all_abs_err_p75', 'all_abs_err_p90', \n",
    "             'all_rsquared', 'all_explain_variance']\n",
    "    print(df_results[df_results['name']=='overall'][means])\n",
    "        \n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_flat_predictions(y_true, y_pred,target_col_list):\n",
    "    result = {}\n",
    "    # Work through overall \n",
    "    y_true_overall = y_true.flatten()\n",
    "    y_pred_overall = y_pred.flatten()    \n",
    "    mae, mape, cmape, smape, ae_p25, ae_p50, ae_p75, ae_p90, rsquared, \\\n",
    "            explain_variance, ape_p75, ape_p90, num_vals = eval_results(y_true_overall, y_pred_overall)\n",
    "    result = {\n",
    "        \"name\": [\"Overall\"],\n",
    "        'mean_y': [np.mean(y_true_overall)],\n",
    "        'mae': [mae],\n",
    "        'mape': [mape],\n",
    "        'cmape': [cmape],\n",
    "        'ae_p75': [ae_p75],\n",
    "        'ae_p90': [ae_p90],\n",
    "        'ape_p75': [ape_p75],\n",
    "        'ape_p90': [ape_p90]\n",
    "    }\n",
    "\n",
    "        \n",
    "    # Overall per output step\n",
    "    print('Overall results per week')\n",
    "    print('-' * 40)\n",
    "    for step in range(y_true.shape[1]):\n",
    "        print('Week', step+1)\n",
    "\n",
    "        y_true_week = y_true[:, step].flatten()\n",
    "        y_pred_week = y_pred[:, step].flatten()\n",
    "        mae, mape, cmape, smape, ae_p25, ae_p50, ae_p75, ae_p90, rsquared, \\\n",
    "            explain_variance, ape_p75, ape_p90, num_vals = eval_results(y_true_week, y_pred_week)\n",
    "\n",
    "        result[str(step+1) + '_mean_y'] = [np.mean(y_true_week)]\n",
    "        result[str(step+1) + '_mae'] = [mae]\n",
    "        result[str(step+1) + '_mape'] = [mape]\n",
    "        result[str(step+1) + '_cmape'] = [cmape]\n",
    "        result[str(step+1) + '_ae_p75'] = [ae_p75]\n",
    "        result[str(step+1) + '_ae_p90'] = [ae_p90]\n",
    "        result[str(step+1) + '_ape_p75'] = [ape_p75]\n",
    "        result[str(step+1) + '_ape_p90'] = [ape_p90]\n",
    "    \n",
    "    # create data frame from results\n",
    "    df_results = pd.DataFrame.from_dict(result)\n",
    "\n",
    "            \n",
    "    \n",
    "    # Each symbol overall\n",
    "    print('-' * 40)\n",
    "    print('Results per target')\n",
    "    for col_num in range(len(target_col_list)):\n",
    "        target_name = target_col_list[col_num]\n",
    "        print(\" \", target_name)\n",
    "        y_true_week = y_true[:, :, col_num].flatten()\n",
    "        y_pred_week = y_pred[:, :, col_num].flatten()\n",
    "        mae, mape, cmape, smape, ae_p25, ae_p50, ae_p75, ae_p90, rsquared, \\\n",
    "            explain_variance, ape_p75, ape_p90, num_vals = eval_results(y_true_week, y_pred_week)\n",
    "\n",
    "        result = {\n",
    "            \"name\": [target_name],\n",
    "            'mean_y': [np.mean(y_true_week)],\n",
    "            'mae': [mae],\n",
    "            'mape': [mape],\n",
    "            'cmape': [cmape],\n",
    "            'ae_p75': [ae_p75],\n",
    "            'ae_p90': [ae_p90],\n",
    "            'ape_p75': [ape_p75],\n",
    "            'ape_p90': [ape_p90]\n",
    "        }\n",
    "\n",
    "\n",
    "        for step in range(y_true.shape[1]):\n",
    "            print('    Week', step+1)\n",
    "            y_true_week = y_true[:, step, col_num].flatten()\n",
    "            y_pred_week = y_pred[:, step, col_num].flatten()\n",
    "            mae, mape, cmape, smape, ae_p25, ae_p50, ae_p75, ae_p90, rsquared, \\\n",
    "                explain_variance, ape_p75, ape_p90, num_vals = eval_results(y_true_week, y_pred_week)\n",
    "\n",
    "            result[str(step+1) + '_mean_y'] = [np.mean(y_true_week)]\n",
    "            result[str(step+1) + '_mae'] = [mae]\n",
    "            result[str(step+1) + '_mape'] = [mape]\n",
    "            result[str(step+1) + '_cmape'] = [cmape]\n",
    "            result[str(step+1) + '_ae_p75'] = [ae_p75]\n",
    "            result[str(step+1) + '_ae_p90'] = [ae_p90]\n",
    "            result[str(step+1) + '_ape_p75'] = [ape_p75]\n",
    "            result[str(step+1) + '_ape_p90'] = [ape_p90]\n",
    "\n",
    "        # create data frame from results\n",
    "        df_current_result = pd.DataFrame.from_dict(result)\n",
    "\n",
    "        # Add data frame into all results\n",
    "        df_results = pd.concat([df_results, df_current_result])\n",
    "        \n",
    "\n",
    "    \n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Distributed LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import LSTM, GRU\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.regularizers import L1L2\n",
    "from keras import optimizers\n",
    "from keras.callbacks import EarlyStopping, CSVLogger, ModelCheckpoint, ReduceLROnPlateau\n",
    "from AdamW import AdamW\n",
    "from stats_operations import *\n",
    "from chrono_lstm import ChronoLSTM\n",
    "\n",
    "# rnn_train_y = rnn_train_y.reshape((rnn_train_y.shape[0], n_outputs, 1))\n",
    "# rnn_test_y = rnn_test_y.reshape((rnn_test_y.shape[0], n_outputs, 1))\n",
    "\n",
    "layers = [2, 2, 2, 2, 2, 2]\n",
    "batch_size = 128\n",
    "num_epochs = 200\n",
    "num_samples = rnn_flat_train_x.shape[0]\n",
    "\n",
    "scaled_layers = []\n",
    "for layer in layers:\n",
    "    scaled_layers.append(max(int(n_features * layer), 1))\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30)\n",
    "# csv_logger = CSVLogger('./logs/nn-training-log.log')\n",
    "checkpointer = ModelCheckpoint(\n",
    "filepath='./weights/nn-weights.hdf5', verbose=0, save_best_only=True)\n",
    "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=15, min_lr=1e-5)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=15, min_lr=1e-5, verbose=1)\n",
    "weight_decay = 0.005 * batch_size / num_samples / num_epochs **0.5\n",
    "\n",
    "print(scaled_layers)\n",
    "print(batch_size)\n",
    "print(num_epochs)\n",
    "print(num_samples)\n",
    "print(weight_decay)\n",
    "print('input steps', n_timesteps)\n",
    "print('output steps', n_outputs)\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "# model.add(ChronoLSTM(scaled_layers[0], \n",
    "#                     (num_epochs*n_timesteps),\n",
    "#                    activation='relu', \n",
    "#                    input_shape=(n_timesteps, n_features)\n",
    "#                     ))\n",
    "model.add(GRU(scaled_layers[0], \n",
    "                   activation='relu', \n",
    "                   input_shape=(n_timesteps, n_features)\n",
    "#                    dropout=0.2\n",
    "#                    kernel_regularizer=L1L2(0.0001, 0.0001)\n",
    "          ))\n",
    "model.add(RepeatVector(n_outputs))\n",
    "\n",
    "# for layer_size in scaled_layers[1:-1]:\n",
    "# model.add(ChronoLSTM(scaled_layers[0], \n",
    "#                      num_epochs*n_timesteps, \n",
    "#                      activation='relu', \n",
    "#                      return_sequences=True))\n",
    "\n",
    "model.add(GRU(scaled_layers[1], \n",
    "               activation='relu', \n",
    "#                dropout=0.2,\n",
    "               return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(scaled_layers[1], activation='relu')))\n",
    "\n",
    "model.add(GRU(scaled_layers[2], \n",
    "               activation='relu', \n",
    "#                dropout=0.2,\n",
    "               return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(scaled_layers[1], activation='relu')))\n",
    "\n",
    "model.add(GRU(scaled_layers[3], \n",
    "               activation='relu', \n",
    "#                dropout=0.2,\n",
    "               return_sequences=True))\n",
    "\n",
    "model.add(GRU(scaled_layers[3], \n",
    "               activation='relu', \n",
    "#                dropout=0.2,\n",
    "               return_sequences=True))\n",
    "\n",
    "model.add(GRU(scaled_layers[5], \n",
    "               activation='relu', \n",
    "#                dropout=0.2,\n",
    "               return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(scaled_layers[2], activation='relu')))\n",
    "model.add(TimeDistributed(Dense(rnn_flat_train_y.shape[2])))\n",
    "\n",
    "# AdamW(weight_decay=weight_decay)\n",
    "model.compile(loss='mae', optimizer=optimizers.RMSprop(lr=0.002), metrics=[k_clipped_mape])\n",
    "print(model.summary())\n",
    "\n",
    "# fit network\n",
    "model.fit(rnn_flat_train_x, \n",
    "          rnn_flat_train_y,\n",
    "          validation_data = (rnn_flat_test_x, rnn_flat_test_y),\n",
    "          epochs=num_epochs, \n",
    "          batch_size=batch_size, \n",
    "          callbacks=[early_stopping, checkpointer, reduce_lr],\n",
    "          verbose=1)\n",
    "\n",
    "model.load_weights('./weights/nn-weights.hdf5')\n",
    "y_flat_test_predictions = model.predict(rnn_flat_test_x)\n",
    "\n",
    "# model_eval = evaluate_results(rnn_test_symbols, y_test_predictions, rnn_test_y)\n",
    "# model_eval.to_csv('./results/janet-time-distributed-%d-%d-%d-epochs.csv' %(n_input, n_out, num_epochs))\n",
    "\n",
    "y_true = rnn_flat_test_y.flatten()\n",
    "y_pred = y_flat_test_predictions.flatten()    \n",
    "mae, mape, cmape, smape, ae_p25, ae_p50, ae_p75, ae_p90, rsquared, \\\n",
    "        explain_variance, ape_p75, ape_p90, num_vals = eval_results(y_true, y_pred)\n",
    "print('num vals:', num_vals)\n",
    "print('mae:', mae)\n",
    "print('mape:', mape)\n",
    "print('cmape:', cmape)\n",
    "print('ae_p75:', ae_p75)\n",
    "print('ae_p90:', ae_p90)\n",
    "print('ape_p75:', ape_p75)\n",
    "print('ape_p90:', ape_p90)\n",
    "\n",
    "df_results = eval_flat_predictions(rnn_flat_test_y, y_flat_test_predictions, target_col_names)\n",
    "df_results.to_csv('./results/time-distributed.csv')\n",
    "\n",
    "print('Done')\n",
    "# model_eval\n",
    "\n",
    "# 16-4, 100 epochs: loss: 8.0383 - val_loss: 5.6601\n",
    "# 16-4, 200 epochs: loss: 7.9807 - val_loss: 5.6143\n",
    "# 12-4, 100 epochs: loss: 7.9533 - val_loss: 5.8089\n",
    "# 20-4, 200 epochs: loss: 8.2571 - val_loss: 6.0177\n",
    "# 8-4, 200 epochs: loss: 7.9358 - val_loss: 5.7033\n",
    "# 16-4, 200 epochs, L1L2 0.0001:\n",
    "# 16-4, 200 epochs, GRU, L1L2 0.001, mae: 5.704350709915161, mape: 72.01476097106934, RMSprop: 2.718229293823242\n",
    "\n",
    "## 57 symbols in 1 row\n",
    "# 16-8, 200 epochs LSTM mape adam,\n",
    "#                         mae: 3.0469394\n",
    "#                         mape: 62.94738054275513\n",
    "#                         cmape: 67.20255613327026\n",
    "#                         smape: 198.20615296059964\n",
    "#                         ae_p25: 0.03171489853411913\n",
    "#                         ae_p50: 1.1797866225242615\n",
    "#                         ae_p75: 3.9442883133888245\n",
    "#                         ae_p90: 7.953948116302489\n",
    "# 16-8, 200 epochs LSTM mae adam,\n",
    "#                         mae: 3.037885\n",
    "#                         mape: 62.2211754322052\n",
    "#                         cmape: 65.83806872367859\n",
    "#                         smape: 198.04303329443397\n",
    "#                         ae_p25: 0.006103984662331641\n",
    "#                         ae_p50: 1.176807165145874\n",
    "#                         ae_p75: 3.981552243232727\n",
    "#                         ae_p90: 7.9905468940734865\n",
    "# 16-8, 200 epochs LSTM mae AdamW,\n",
    "#                         mae: 3.0392604\n",
    "#                         mape: 62.26362586021423\n",
    "#                         cmape: 65.8830463886261\n",
    "#                         smape: 198.74643603440208\n",
    "#                         ae_p25: 0.00881427968852222\n",
    "#                         ae_p50: 1.1775119304656982\n",
    "#                         ae_p75: 3.969123423099518\n",
    "#                         ae_p90: 7.973479557037353\n",
    "# 16-8, 200 epochs LSTM mae RMSprop,\n",
    "#                         mae: 3.0371118\n",
    "#                         mape: 62.067198753356934\n",
    "#                         cmape: 65.49670696258545\n",
    "#                         smape: 198.74755064508935\n",
    "#                         ae_p25: 0.0002485034929122776\n",
    "#                         ae_p50: 1.176451325416565\n",
    "#                         ae_p75: 3.9679842591285706\n",
    "#                         ae_p90: 7.998835563659668\n",
    "# 16-8, 200 epochs LSTM mae RMSprop (4, 4 layers),\n",
    "#                         mae: 3.0378606\n",
    "#                         mape: 62.15248107910156\n",
    "#                         cmape: 65.67736864089966\n",
    "#                         smape: 198.5776040141972\n",
    "#                         ae_p25: 0.00444786436855793\n",
    "#                         ae_p50: 1.1738138794898987\n",
    "#                         ae_p75: 3.979398727416992\n",
    "#                         ae_p90: 7.9832131385803216\n",
    "# 16-8, 200 epochs LSTM mae RMSprop (2, 2, 2 layers),\n",
    "#                         mae: 3.037165\n",
    "#                         mape: 62.07066774368286\n",
    "#                         cmape: 65.5011773109436\n",
    "#                         smape: 198.75001840814292\n",
    "#                         ae_p25: 3.774190554395318e-05\n",
    "#                         ae_p50: 1.1764426827430725\n",
    "#                         ae_p75: 3.9704267382621765\n",
    "#                         ae_p90: 7.99989161491394\n",
    "# 16-8, 200 epochs GRU mae RMSprop (2, 2, 2 layers),\n",
    "#                         mae: 3.036734\n",
    "#                         mape: 62.17632293701172\n",
    "#                         cmape: 65.78048467636108\n",
    "#                         smape: 197.84315421789358\n",
    "#                         ae_p25: 0.001321911724517122\n",
    "#                         ae_p50: 1.1769254803657532\n",
    "#                         ae_p75: 3.9657304286956787\n",
    "#                         ae_p90: 7.975777339935299\n",
    "# 16-8, 200 epochs GRU mae RMSprop (2, 2, 2, 2 layers),\n",
    "#                         mae: 3.0370674\n",
    "#                         mape: 62.087345123291016\n",
    "#                         cmape: 65.54595828056335\n",
    "#                         smape: 198.62712994060067\n",
    "#                         ae_p25: 0.0003695989726111293\n",
    "#                         ae_p50: 1.1766981482505798\n",
    "#                         ae_p75: 3.9665741324424744\n",
    "#                         ae_p90: 7.995802545547484\n",
    "# 16-8, 200 epochs GRU mae RMSprop (2, 2, 2, 2, 2 layers),\n",
    "#                         mae: 3.0367754\n",
    "#                         mape: 62.0708703994751\n",
    "#                         cmape: 65.51564931869507\n",
    "#                         smape: 198.7152693335687\n",
    "#                         ae_p25: 0.00028848367219325155\n",
    "#                         ae_p50: 1.1766218543052673\n",
    "#                         ae_p75: 3.9676144123077393\n",
    "#                         ae_p90: 7.9992693901062015\n",
    "# 16-8, 200 epochs GRU mae RMSprop(lr=0.005) (2, 2, 2, 2, 2, 2 layers),\n",
    "#                         mae: 3.0516613\n",
    "#                         mape: 63.53962421417236\n",
    "#                         cmape: 67.20293760299683\n",
    "#                         smape: 196.93598114939047\n",
    "#                         ae_p25: 0.015180134447291493\n",
    "#                         ae_p50: 1.2153788208961487\n",
    "#                         ae_p75: 3.972410023212433\n",
    "#                         ae_p90: 7.973083543777466\n",
    "# 16-8, 200 epochs GRU mae RMSprop(lr=0.002) (2, 2, 2, 2, 2, 2 layers),\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq Encoder - Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder layers [3428, 3428, 1714, 1714]\n",
      "decoder layers [1714, 1714, 3428, 3428]\n",
      "batch_size 128\n",
      "epochs 200\n",
      "Train shape\n",
      "468 , 16 , 1714\n",
      "(468, 16, 1714)\n",
      "Test shape\n",
      "468 , 8 ,  113\n",
      "(468, 8, 113)\n",
      "input steps 16\n",
      "input features 1714\n",
      "output steps 8\n",
      "output features 113\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_13 (InputLayer)           (None, 16, 1714)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gru_36 (GRU)                    (None, 3428)         52890612    input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_7 (RepeatVector)  (None, 8, 3428)      0           gru_36[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "gru_37 (GRU)                    (None, 8, 3428)      70517388    repeat_vector_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_28 (TimeDistri (None, 8, 3428)      11754612    gru_37[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "gru_38 (GRU)                    (None, 8, 1714)      26445306    time_distributed_28[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_29 (TimeDistri (None, 8, 1714)      2939510     gru_38[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "input_14 (InputLayer)           (None, 8, 113)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gru_39 (GRU)                    [(None, 8, 1714), (N 17631918    time_distributed_29[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "gru_40 (GRU)                    [(None, 8, 1714), (N 9399576     input_14[0][0]                   \n",
      "                                                                 gru_39[0][1]                     \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_30 (TimeDistri (None, 8, 1714)      2939510     gru_40[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "gru_41 (GRU)                    (None, 8, 1714)      17631918    time_distributed_30[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_31 (TimeDistri (None, 8, 1714)      2939510     gru_41[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "gru_42 (GRU)                    (None, 8, 3428)      52890612    time_distributed_31[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_32 (TimeDistri (None, 8, 3428)      11754612    gru_42[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "gru_43 (GRU)                    (None, 8, 3428)      70517388    time_distributed_32[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_33 (TimeDistri (None, 8, 3428)      11754612    gru_43[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_34 (TimeDistri (None, 8, 113)       387477      time_distributed_33[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 362,394,561\n",
      "Trainable params: 362,394,561\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train on 468 samples, validate on 110 samples\n",
      "Epoch 1/200\n",
      "1/1 [==============================] - 358s 358s/step - loss: 4.1057 - k_mean_absolute_percentage_error: 52.6601 - k_mae_mape: 2.2713 - val_loss: 3.8364 - val_k_mean_absolute_percentage_error: 181.3084 - val_k_mae_mape: 19.7212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/neilkloot/anaconda/lib/python3.6/site-packages/keras/engine/network.py:888: UserWarning: Layer gru_40 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'gru_39/while/Exit_3:0' shape=(?, 1714) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/200\n",
      "1/1 [==============================] - 60s 60s/step - loss: 6.1085 - k_mean_absolute_percentage_error: 249.1408 - k_mae_mape: 49.5902 - val_loss: 1440459.6250 - val_k_mean_absolute_percentage_error: 100956576.0000 - val_k_mae_mape: 6025924050944.0000\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 63s 63s/step - loss: 1687020.2500 - k_mean_absolute_percentage_error: 114424632.0000 - k_mae_mape: 8892883927040.0000 - val_loss: 251131488.0000 - val_k_mean_absolute_percentage_error: 17509703680.0000 - val_k_mae_mape: 209402934402744320.0000\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 77s 77s/step - loss: 344384480.0000 - k_mean_absolute_percentage_error: 22665371648.0000 - k_mae_mape: 389567037124378624.0000 - val_loss: 4.7167 - val_k_mean_absolute_percentage_error: 254.2543 - val_k_mae_mape: 34.7143\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 70s 70s/step - loss: 6.9156 - k_mean_absolute_percentage_error: 300.9145 - k_mae_mape: 63.3765 - val_loss: 315.3219 - val_k_mean_absolute_percentage_error: 22069.7637 - val_k_mae_mape: 279920.6875\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 71s 71s/step - loss: 427.1803 - k_mean_absolute_percentage_error: 28320.4023 - k_mae_mape: 500485.6875 - val_loss: 277.8655 - val_k_mean_absolute_percentage_error: 19444.5371 - val_k_mae_mape: 186304.1406\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 66s 66s/step - loss: 371.4205 - k_mean_absolute_percentage_error: 24547.5703 - k_mae_mape: 320186.5000 - val_loss: 2.8583 - val_k_mean_absolute_percentage_error: 89.9801 - val_k_mae_mape: 5.3309\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 76s 76s/step - loss: 4.6800 - k_mean_absolute_percentage_error: 109.6016 - k_mae_mape: 12.3630 - val_loss: 5.2288 - val_k_mean_absolute_percentage_error: 292.9304 - val_k_mae_mape: 50.1394\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 70s 70s/step - loss: 8.2042 - k_mean_absolute_percentage_error: 405.8965 - k_mae_mape: 110.7627 - val_loss: 2.6992 - val_k_mean_absolute_percentage_error: 74.4838 - val_k_mae_mape: 3.5672\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 79s 79s/step - loss: 4.3621 - k_mean_absolute_percentage_error: 78.9531 - k_mae_mape: 6.8545 - val_loss: 2.7551 - val_k_mean_absolute_percentage_error: 79.9238 - val_k_mae_mape: 4.0712\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 81s 81s/step - loss: 4.4276 - k_mean_absolute_percentage_error: 84.5952 - k_mae_mape: 7.8559 - val_loss: 3.0483 - val_k_mean_absolute_percentage_error: 108.0683 - val_k_mae_mape: 7.5486\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 64s 64s/step - loss: 4.8882 - k_mean_absolute_percentage_error: 130.8280 - k_mae_mape: 17.1011 - val_loss: 2.5367 - val_k_mean_absolute_percentage_error: 58.0084 - val_k_mae_mape: 1.6766\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 60s 60s/step - loss: 4.1146 - k_mean_absolute_percentage_error: 53.5851 - k_mae_mape: 2.4304 - val_loss: 3.0048 - val_k_mean_absolute_percentage_error: 104.2275 - val_k_mae_mape: 7.0911\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 62s 62s/step - loss: 4.7046 - k_mean_absolute_percentage_error: 111.5384 - k_mae_mape: 12.9168 - val_loss: 2.5476 - val_k_mean_absolute_percentage_error: 59.1038 - val_k_mae_mape: 1.7967\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 62s 62s/step - loss: 4.1293 - k_mean_absolute_percentage_error: 55.1001 - k_mae_mape: 2.6844 - val_loss: 2.5360 - val_k_mean_absolute_percentage_error: 57.9414 - val_k_mae_mape: 1.6702\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 60s 60s/step - loss: 4.1147 - k_mean_absolute_percentage_error: 53.5958 - k_mae_mape: 2.4313 - val_loss: 2.5312 - val_k_mean_absolute_percentage_error: 57.4532 - val_k_mae_mape: 1.6174\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 60s 60s/step - loss: 4.1082 - k_mean_absolute_percentage_error: 52.9260 - k_mae_mape: 2.3171 - val_loss: 2.5334 - val_k_mean_absolute_percentage_error: 57.6588 - val_k_mae_mape: 1.6398\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 61s 61s/step - loss: 4.1123 - k_mean_absolute_percentage_error: 53.3418 - k_mae_mape: 2.3878 - val_loss: 2.5310 - val_k_mean_absolute_percentage_error: 57.4288 - val_k_mae_mape: 1.6147\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 59s 59s/step - loss: 4.1081 - k_mean_absolute_percentage_error: 52.9200 - k_mae_mape: 2.3162 - val_loss: 2.5300 - val_k_mean_absolute_percentage_error: 57.3297 - val_k_mae_mape: 1.6045\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 59s 59s/step - loss: 4.1074 - k_mean_absolute_percentage_error: 52.8416 - k_mae_mape: 2.3023 - val_loss: 2.5305 - val_k_mean_absolute_percentage_error: 57.3787 - val_k_mae_mape: 1.6091\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 60s 60s/step - loss: 4.1076 - k_mean_absolute_percentage_error: 52.8620 - k_mae_mape: 2.3061 - val_loss: 2.5297 - val_k_mean_absolute_percentage_error: 57.2966 - val_k_mae_mape: 1.6007\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 59s 59s/step - loss: 4.1066 - k_mean_absolute_percentage_error: 52.7603 - k_mae_mape: 2.2889 - val_loss: 2.5297 - val_k_mean_absolute_percentage_error: 57.2948 - val_k_mae_mape: 1.6005\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 58s 58s/step - loss: 4.1067 - k_mean_absolute_percentage_error: 52.7696 - k_mae_mape: 2.2905 - val_loss: 2.5295 - val_k_mean_absolute_percentage_error: 57.2789 - val_k_mae_mape: 1.5989\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 58s 58s/step - loss: 4.1065 - k_mean_absolute_percentage_error: 52.7497 - k_mae_mape: 2.2870 - val_loss: 2.5293 - val_k_mean_absolute_percentage_error: 57.2580 - val_k_mae_mape: 1.5966\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 58s 58s/step - loss: 4.1062 - k_mean_absolute_percentage_error: 52.7172 - k_mae_mape: 2.2816 - val_loss: 2.5295 - val_k_mean_absolute_percentage_error: 57.2756 - val_k_mae_mape: 1.5985\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 59s 59s/step - loss: 4.1065 - k_mean_absolute_percentage_error: 52.7437 - k_mae_mape: 2.2859 - val_loss: 2.5293 - val_k_mean_absolute_percentage_error: 57.2564 - val_k_mae_mape: 1.5964\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 58s 58s/step - loss: 4.1062 - k_mean_absolute_percentage_error: 52.7185 - k_mae_mape: 2.2819 - val_loss: 2.5294 - val_k_mean_absolute_percentage_error: 57.2666 - val_k_mae_mape: 1.5975\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 58s 58s/step - loss: 4.1064 - k_mean_absolute_percentage_error: 52.7332 - k_mae_mape: 2.2843 - val_loss: 2.5293 - val_k_mean_absolute_percentage_error: 57.2566 - val_k_mae_mape: 1.5964\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 58s 58s/step - loss: 4.1062 - k_mean_absolute_percentage_error: 52.7178 - k_mae_mape: 2.2817 - val_loss: 2.5294 - val_k_mean_absolute_percentage_error: 57.2661 - val_k_mae_mape: 1.5974\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 59s 59s/step - loss: 4.1063 - k_mean_absolute_percentage_error: 52.7294 - k_mae_mape: 2.2836 - val_loss: 2.5293 - val_k_mean_absolute_percentage_error: 57.2590 - val_k_mae_mape: 1.5967\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 71s 71s/step - loss: 4.1062 - k_mean_absolute_percentage_error: 52.7179 - k_mae_mape: 2.2818 - val_loss: 2.5293 - val_k_mean_absolute_percentage_error: 57.2606 - val_k_mae_mape: 1.5968\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 74s 74s/step - loss: 4.1063 - k_mean_absolute_percentage_error: 52.7255 - k_mae_mape: 2.2829 - val_loss: 2.5293 - val_k_mean_absolute_percentage_error: 57.2580 - val_k_mae_mape: 1.5966\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 73s 73s/step - loss: 4.1062 - k_mean_absolute_percentage_error: 52.7182 - k_mae_mape: 2.2818 - val_loss: 2.5294 - val_k_mean_absolute_percentage_error: 57.2645 - val_k_mae_mape: 1.5973\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 74s 74s/step - loss: 4.1062 - k_mean_absolute_percentage_error: 52.7218 - k_mae_mape: 2.2823 - val_loss: 2.5293 - val_k_mean_absolute_percentage_error: 57.2569 - val_k_mae_mape: 1.5965\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 73s 73s/step - loss: 4.1062 - k_mean_absolute_percentage_error: 52.7179 - k_mae_mape: 2.2818 - val_loss: 2.5293 - val_k_mean_absolute_percentage_error: 57.2594 - val_k_mae_mape: 1.5967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/200\n",
      "1/1 [==============================] - 73s 73s/step - loss: 4.1062 - k_mean_absolute_percentage_error: 52.7171 - k_mae_mape: 2.2815 - val_loss: 2.5293 - val_k_mean_absolute_percentage_error: 57.2612 - val_k_mae_mape: 1.5969\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 74s 74s/step - loss: 4.1062 - k_mean_absolute_percentage_error: 52.7202 - k_mae_mape: 2.2823 - val_loss: 2.5293 - val_k_mean_absolute_percentage_error: 57.2588 - val_k_mae_mape: 1.5967\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 74s 74s/step - loss: 4.1062 - k_mean_absolute_percentage_error: 52.7174 - k_mae_mape: 2.2815 - val_loss: 2.5293 - val_k_mean_absolute_percentage_error: 57.2575 - val_k_mae_mape: 1.5965\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 74s 74s/step - loss: 4.1062 - k_mean_absolute_percentage_error: 52.7171 - k_mae_mape: 2.2817 - val_loss: 2.5294 - val_k_mean_absolute_percentage_error: 57.2639 - val_k_mae_mape: 1.5973\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 91s 91s/step - loss: 4.1062 - k_mean_absolute_percentage_error: 52.7222 - k_mae_mape: 2.2824 - val_loss: 2.5291 - val_k_mean_absolute_percentage_error: 57.2402 - val_k_mae_mape: 1.5948\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 72s 72s/step - loss: 4.1059 - k_mean_absolute_percentage_error: 52.6897 - k_mae_mape: 2.2770 - val_loss: 2.5290 - val_k_mean_absolute_percentage_error: 57.2327 - val_k_mae_mape: 1.5938\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 70s 70s/step - loss: 4.1059 - k_mean_absolute_percentage_error: 52.6846 - k_mae_mape: 2.2760 - val_loss: 2.5291 - val_k_mean_absolute_percentage_error: 57.2374 - val_k_mae_mape: 1.5944\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 60s 60s/step - loss: 4.1059 - k_mean_absolute_percentage_error: 52.6894 - k_mae_mape: 2.2769 - val_loss: 2.5291 - val_k_mean_absolute_percentage_error: 57.2343 - val_k_mae_mape: 1.5940\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 58s 58s/step - loss: 4.1059 - k_mean_absolute_percentage_error: 52.6868 - k_mae_mape: 2.2765 - val_loss: 2.5291 - val_k_mean_absolute_percentage_error: 57.2364 - val_k_mae_mape: 1.5943\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 59s 59s/step - loss: 4.1059 - k_mean_absolute_percentage_error: 52.6887 - k_mae_mape: 2.2767 - val_loss: 2.5291 - val_k_mean_absolute_percentage_error: 57.2361 - val_k_mae_mape: 1.5942\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 59s 59s/step - loss: 4.1059 - k_mean_absolute_percentage_error: 52.6890 - k_mae_mape: 2.2769 - val_loss: 2.5291 - val_k_mean_absolute_percentage_error: 57.2347 - val_k_mae_mape: 1.5941\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 58s 58s/step - loss: 4.1059 - k_mean_absolute_percentage_error: 52.6859 - k_mae_mape: 2.2763 - val_loss: 2.5291 - val_k_mean_absolute_percentage_error: 57.2364 - val_k_mae_mape: 1.5942\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 58s 58s/step - loss: 4.1059 - k_mean_absolute_percentage_error: 52.6898 - k_mae_mape: 2.2771 - val_loss: 2.5290 - val_k_mean_absolute_percentage_error: 57.2334 - val_k_mae_mape: 1.5940\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 58s 58s/step - loss: 4.1059 - k_mean_absolute_percentage_error: 52.6847 - k_mae_mape: 2.2761 - val_loss: 2.5291 - val_k_mean_absolute_percentage_error: 57.2377 - val_k_mae_mape: 1.5944\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 58s 58s/step - loss: 4.1059 - k_mean_absolute_percentage_error: 52.6897 - k_mae_mape: 2.2770 - val_loss: 2.5291 - val_k_mean_absolute_percentage_error: 57.2397 - val_k_mae_mape: 1.5947\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 59s 59s/step - loss: 4.1059 - k_mean_absolute_percentage_error: 52.6885 - k_mae_mape: 2.2768 - val_loss: 2.5291 - val_k_mean_absolute_percentage_error: 57.2371 - val_k_mae_mape: 1.5944\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 58s 58s/step - loss: 4.1059 - k_mean_absolute_percentage_error: 52.6873 - k_mae_mape: 2.2766 - val_loss: 2.5291 - val_k_mean_absolute_percentage_error: 57.2410 - val_k_mae_mape: 1.5948\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 58s 58s/step - loss: 4.1059 - k_mean_absolute_percentage_error: 52.6896 - k_mae_mape: 2.2770 - val_loss: 2.5291 - val_k_mean_absolute_percentage_error: 57.2375 - val_k_mae_mape: 1.5944\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 58s 58s/step - loss: 4.1059 - k_mean_absolute_percentage_error: 52.6874 - k_mae_mape: 2.2767 - val_loss: 2.5291 - val_k_mean_absolute_percentage_error: 57.2384 - val_k_mae_mape: 1.5946\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 59s 59s/step - loss: 4.1059 - k_mean_absolute_percentage_error: 52.6901 - k_mae_mape: 2.2771 - val_loss: 2.5290 - val_k_mean_absolute_percentage_error: 57.2324 - val_k_mae_mape: 1.5939\n",
      "\n",
      "Epoch 00055: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 57s 57s/step - loss: 4.1059 - k_mean_absolute_percentage_error: 52.6847 - k_mae_mape: 2.2763 - val_loss: 2.5289 - val_k_mean_absolute_percentage_error: 57.2240 - val_k_mae_mape: 1.5930\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 58s 58s/step - loss: 4.1057 - k_mean_absolute_percentage_error: 52.6707 - k_mae_mape: 2.2739 - val_loss: 2.5290 - val_k_mean_absolute_percentage_error: 57.2242 - val_k_mae_mape: 1.5930\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 57s 57s/step - loss: 4.1058 - k_mean_absolute_percentage_error: 52.6724 - k_mae_mape: 2.2742 - val_loss: 2.5290 - val_k_mean_absolute_percentage_error: 57.2260 - val_k_mae_mape: 1.5932\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 58s 58s/step - loss: 4.1058 - k_mean_absolute_percentage_error: 52.6747 - k_mae_mape: 2.2746 - val_loss: 2.5289 - val_k_mean_absolute_percentage_error: 57.2228 - val_k_mae_mape: 1.5929\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 58s 58s/step - loss: 4.1057 - k_mean_absolute_percentage_error: 52.6708 - k_mae_mape: 2.2739 - val_loss: 2.5290 - val_k_mean_absolute_percentage_error: 57.2269 - val_k_mae_mape: 1.5933\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 58s 58s/step - loss: 4.1058 - k_mean_absolute_percentage_error: 52.6749 - k_mae_mape: 2.2746 - val_loss: 2.5289 - val_k_mean_absolute_percentage_error: 57.2234 - val_k_mae_mape: 1.5929\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 64s 64s/step - loss: 4.1058 - k_mean_absolute_percentage_error: 52.6724 - k_mae_mape: 2.2742 - val_loss: 2.5290 - val_k_mean_absolute_percentage_error: 57.2271 - val_k_mae_mape: 1.5933\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 71s 71s/step - loss: 4.1058 - k_mean_absolute_percentage_error: 52.6754 - k_mae_mape: 2.2747 - val_loss: 2.5289 - val_k_mean_absolute_percentage_error: 57.2235 - val_k_mae_mape: 1.5930\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 71s 71s/step - loss: 4.1057 - k_mean_absolute_percentage_error: 52.6717 - k_mae_mape: 2.2741 - val_loss: 2.5290 - val_k_mean_absolute_percentage_error: 57.2258 - val_k_mae_mape: 1.5932\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 73s 73s/step - loss: 4.1058 - k_mean_absolute_percentage_error: 52.6740 - k_mae_mape: 2.2745 - val_loss: 2.5289 - val_k_mean_absolute_percentage_error: 57.2233 - val_k_mae_mape: 1.5930\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 73s 73s/step - loss: 4.1057 - k_mean_absolute_percentage_error: 52.6704 - k_mae_mape: 2.2739 - val_loss: 2.5290 - val_k_mean_absolute_percentage_error: 57.2247 - val_k_mae_mape: 1.5931\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 71s 71s/step - loss: 4.1058 - k_mean_absolute_percentage_error: 52.6732 - k_mae_mape: 2.2744 - val_loss: 2.5289 - val_k_mean_absolute_percentage_error: 57.2241 - val_k_mae_mape: 1.5931\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 72s 72s/step - loss: 4.1057 - k_mean_absolute_percentage_error: 52.6721 - k_mae_mape: 2.2741 - val_loss: 2.5290 - val_k_mean_absolute_percentage_error: 57.2273 - val_k_mae_mape: 1.5934\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 71s 71s/step - loss: 4.1058 - k_mean_absolute_percentage_error: 52.6746 - k_mae_mape: 2.2746 - val_loss: 2.5289 - val_k_mean_absolute_percentage_error: 57.2237 - val_k_mae_mape: 1.5930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/200\n",
      "1/1 [==============================] - 73s 73s/step - loss: 4.1057 - k_mean_absolute_percentage_error: 52.6717 - k_mae_mape: 2.2741 - val_loss: 2.5290 - val_k_mean_absolute_percentage_error: 57.2261 - val_k_mae_mape: 1.5932\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 71s 71s/step - loss: 4.1058 - k_mean_absolute_percentage_error: 52.6740 - k_mae_mape: 2.2745 - val_loss: 2.5290 - val_k_mean_absolute_percentage_error: 57.2255 - val_k_mae_mape: 1.5932\n",
      "\n",
      "Epoch 00071: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 73s 73s/step - loss: 4.1058 - k_mean_absolute_percentage_error: 52.6731 - k_mae_mape: 2.2744 - val_loss: 2.5289 - val_k_mean_absolute_percentage_error: 57.2193 - val_k_mae_mape: 1.5926\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 71s 71s/step - loss: 4.1057 - k_mean_absolute_percentage_error: 52.6645 - k_mae_mape: 2.2729 - val_loss: 2.5289 - val_k_mean_absolute_percentage_error: 57.2171 - val_k_mae_mape: 1.5923\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 71s 71s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6622 - k_mae_mape: 2.2725 - val_loss: 2.5289 - val_k_mean_absolute_percentage_error: 57.2195 - val_k_mae_mape: 1.5925\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 72s 72s/step - loss: 4.1057 - k_mean_absolute_percentage_error: 52.6654 - k_mae_mape: 2.2731 - val_loss: 2.5289 - val_k_mean_absolute_percentage_error: 57.2185 - val_k_mae_mape: 1.5925\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 72s 72s/step - loss: 4.1057 - k_mean_absolute_percentage_error: 52.6642 - k_mae_mape: 2.2728 - val_loss: 2.5289 - val_k_mean_absolute_percentage_error: 57.2197 - val_k_mae_mape: 1.5926\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 71s 71s/step - loss: 4.1057 - k_mean_absolute_percentage_error: 52.6654 - k_mae_mape: 2.2731 - val_loss: 2.5289 - val_k_mean_absolute_percentage_error: 57.2174 - val_k_mae_mape: 1.5923\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 71s 71s/step - loss: 4.1057 - k_mean_absolute_percentage_error: 52.6629 - k_mae_mape: 2.2726 - val_loss: 2.5289 - val_k_mean_absolute_percentage_error: 57.2199 - val_k_mae_mape: 1.5926\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 72s 72s/step - loss: 4.1057 - k_mean_absolute_percentage_error: 52.6656 - k_mae_mape: 2.2731 - val_loss: 2.5289 - val_k_mean_absolute_percentage_error: 57.2180 - val_k_mae_mape: 1.5924\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 71s 71s/step - loss: 4.1057 - k_mean_absolute_percentage_error: 52.6639 - k_mae_mape: 2.2728 - val_loss: 2.5289 - val_k_mean_absolute_percentage_error: 57.2193 - val_k_mae_mape: 1.5925\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 71s 71s/step - loss: 4.1057 - k_mean_absolute_percentage_error: 52.6652 - k_mae_mape: 2.2731 - val_loss: 2.5289 - val_k_mean_absolute_percentage_error: 57.2182 - val_k_mae_mape: 1.5924\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 72s 72s/step - loss: 4.1057 - k_mean_absolute_percentage_error: 52.6636 - k_mae_mape: 2.2727 - val_loss: 2.5289 - val_k_mean_absolute_percentage_error: 57.2203 - val_k_mae_mape: 1.5926\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 71s 71s/step - loss: 4.1057 - k_mean_absolute_percentage_error: 52.6658 - k_mae_mape: 2.2732 - val_loss: 2.5289 - val_k_mean_absolute_percentage_error: 57.2180 - val_k_mae_mape: 1.5924\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 71s 71s/step - loss: 4.1057 - k_mean_absolute_percentage_error: 52.6630 - k_mae_mape: 2.2727 - val_loss: 2.5289 - val_k_mean_absolute_percentage_error: 57.2209 - val_k_mae_mape: 1.5927\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 71s 71s/step - loss: 4.1057 - k_mean_absolute_percentage_error: 52.6666 - k_mae_mape: 2.2733 - val_loss: 2.5289 - val_k_mean_absolute_percentage_error: 57.2189 - val_k_mae_mape: 1.5925\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 71s 71s/step - loss: 4.1057 - k_mean_absolute_percentage_error: 52.6642 - k_mae_mape: 2.2729 - val_loss: 2.5289 - val_k_mean_absolute_percentage_error: 57.2200 - val_k_mae_mape: 1.5926\n",
      "\n",
      "Epoch 00086: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 72s 72s/step - loss: 4.1057 - k_mean_absolute_percentage_error: 52.6658 - k_mae_mape: 2.2732 - val_loss: 2.5289 - val_k_mean_absolute_percentage_error: 57.2162 - val_k_mae_mape: 1.5922\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 71s 71s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6605 - k_mae_mape: 2.2723 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2149 - val_k_mae_mape: 1.5921\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 71s 71s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6590 - k_mae_mape: 2.2720 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2154 - val_k_mae_mape: 1.5921\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 71s 71s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6597 - k_mae_mape: 2.2721 - val_loss: 2.5289 - val_k_mean_absolute_percentage_error: 57.2156 - val_k_mae_mape: 1.5922\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 71s 71s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6601 - k_mae_mape: 2.2722 - val_loss: 2.5289 - val_k_mean_absolute_percentage_error: 57.2161 - val_k_mae_mape: 1.5922\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 71s 71s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6612 - k_mae_mape: 2.2724 - val_loss: 2.5289 - val_k_mean_absolute_percentage_error: 57.2165 - val_k_mae_mape: 1.5922\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 71s 71s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6605 - k_mae_mape: 2.2723 - val_loss: 2.5289 - val_k_mean_absolute_percentage_error: 57.2161 - val_k_mae_mape: 1.5922\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 71s 71s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6613 - k_mae_mape: 2.2724 - val_loss: 2.5289 - val_k_mean_absolute_percentage_error: 57.2164 - val_k_mae_mape: 1.5922\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 71s 71s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6608 - k_mae_mape: 2.2723 - val_loss: 2.5289 - val_k_mean_absolute_percentage_error: 57.2159 - val_k_mae_mape: 1.5922\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 71s 71s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6606 - k_mae_mape: 2.2723 - val_loss: 2.5289 - val_k_mean_absolute_percentage_error: 57.2166 - val_k_mae_mape: 1.5923\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 70s 70s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6609 - k_mae_mape: 2.2723 - val_loss: 2.5289 - val_k_mean_absolute_percentage_error: 57.2160 - val_k_mae_mape: 1.5922\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 71s 71s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6609 - k_mae_mape: 2.2723 - val_loss: 2.5289 - val_k_mean_absolute_percentage_error: 57.2163 - val_k_mae_mape: 1.5922\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 71s 71s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6605 - k_mae_mape: 2.2723 - val_loss: 2.5289 - val_k_mean_absolute_percentage_error: 57.2160 - val_k_mae_mape: 1.5922\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 71s 71s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6607 - k_mae_mape: 2.2723 - val_loss: 2.5289 - val_k_mean_absolute_percentage_error: 57.2166 - val_k_mae_mape: 1.5923\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 71s 71s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6607 - k_mae_mape: 2.2723 - val_loss: 2.5289 - val_k_mean_absolute_percentage_error: 57.2162 - val_k_mae_mape: 1.5922\n",
      "\n",
      "Epoch 00101: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 71s 71s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6607 - k_mae_mape: 2.2723 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2147 - val_k_mae_mape: 1.5921\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 71s 71s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6584 - k_mae_mape: 2.2719 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2141 - val_k_mae_mape: 1.5920\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104/200\n",
      "1/1 [==============================] - 72s 72s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6579 - k_mae_mape: 2.2718 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2143 - val_k_mae_mape: 1.5920\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 71s 71s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6580 - k_mae_mape: 2.2719 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2143 - val_k_mae_mape: 1.5920\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 71s 71s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6582 - k_mae_mape: 2.2719 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2145 - val_k_mae_mape: 1.5920\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 71s 71s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6582 - k_mae_mape: 2.2719 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2141 - val_k_mae_mape: 1.5920\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 70s 70s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6580 - k_mae_mape: 2.2718 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2146 - val_k_mae_mape: 1.5921\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 70s 70s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6585 - k_mae_mape: 2.2719 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2145 - val_k_mae_mape: 1.5921\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 70s 70s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6585 - k_mae_mape: 2.2719 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2148 - val_k_mae_mape: 1.5921\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 70s 70s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6586 - k_mae_mape: 2.2720 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2146 - val_k_mae_mape: 1.5921\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 70s 70s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6585 - k_mae_mape: 2.2719 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2148 - val_k_mae_mape: 1.5921\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 70s 70s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6585 - k_mae_mape: 2.2719 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2146 - val_k_mae_mape: 1.5921\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 70s 70s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6586 - k_mae_mape: 2.2720 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2146 - val_k_mae_mape: 1.5921\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 71s 71s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6584 - k_mae_mape: 2.2719 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2145 - val_k_mae_mape: 1.5921\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 70s 70s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6584 - k_mae_mape: 2.2719 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2146 - val_k_mae_mape: 1.5921\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 70s 70s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6585 - k_mae_mape: 2.2719 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2146 - val_k_mae_mape: 1.5921\n",
      "\n",
      "Epoch 00117: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 70s 70s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6584 - k_mae_mape: 2.2719 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2139 - val_k_mae_mape: 1.5920\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 70s 70s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6574 - k_mae_mape: 2.2718 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2136 - val_k_mae_mape: 1.5920\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 70s 70s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6571 - k_mae_mape: 2.2717 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2137 - val_k_mae_mape: 1.5920\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 70s 70s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6571 - k_mae_mape: 2.2717 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2136 - val_k_mae_mape: 1.5920\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 69s 69s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6570 - k_mae_mape: 2.2717 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2138 - val_k_mae_mape: 1.5920\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 70s 70s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6571 - k_mae_mape: 2.2717 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2137 - val_k_mae_mape: 1.5920\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 70s 70s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6572 - k_mae_mape: 2.2717 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2138 - val_k_mae_mape: 1.5920\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 70s 70s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6572 - k_mae_mape: 2.2717 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2137 - val_k_mae_mape: 1.5920\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 70s 70s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6571 - k_mae_mape: 2.2717 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2138 - val_k_mae_mape: 1.5920\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 70s 70s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6572 - k_mae_mape: 2.2717 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2139 - val_k_mae_mape: 1.5920\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 70s 70s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6573 - k_mae_mape: 2.2717 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2138 - val_k_mae_mape: 1.5920\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 70s 70s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6574 - k_mae_mape: 2.2718 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2141 - val_k_mae_mape: 1.5920\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 70s 70s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6574 - k_mae_mape: 2.2718 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2137 - val_k_mae_mape: 1.5920\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 70s 70s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6574 - k_mae_mape: 2.2717 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2139 - val_k_mae_mape: 1.5920\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 70s 70s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6573 - k_mae_mape: 2.2717 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2137 - val_k_mae_mape: 1.5920\n",
      "\n",
      "Epoch 00132: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 70s 70s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6572 - k_mae_mape: 2.2717 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2135 - val_k_mae_mape: 1.5920\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 69s 69s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6568 - k_mae_mape: 2.2717 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2135 - val_k_mae_mape: 1.5919\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 69s 69s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6567 - k_mae_mape: 2.2716 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2135 - val_k_mae_mape: 1.5919\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 69s 69s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6567 - k_mae_mape: 2.2716 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2135 - val_k_mae_mape: 1.5919\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 69s 69s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6567 - k_mae_mape: 2.2716 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2136 - val_k_mae_mape: 1.5920\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 138/200\n",
      "1/1 [==============================] - 69s 69s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6568 - k_mae_mape: 2.2717 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2135 - val_k_mae_mape: 1.5920\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 69s 69s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6569 - k_mae_mape: 2.2717 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2136 - val_k_mae_mape: 1.5920\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 70s 70s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6569 - k_mae_mape: 2.2717 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2135 - val_k_mae_mape: 1.5919\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 70s 70s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6569 - k_mae_mape: 2.2717 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2136 - val_k_mae_mape: 1.5920\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 69s 69s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6568 - k_mae_mape: 2.2717 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2135 - val_k_mae_mape: 1.5919\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 70s 70s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6568 - k_mae_mape: 2.2717 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2136 - val_k_mae_mape: 1.5920\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 69s 69s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6568 - k_mae_mape: 2.2717 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2134 - val_k_mae_mape: 1.5919\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 69s 69s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6568 - k_mae_mape: 2.2717 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2136 - val_k_mae_mape: 1.5920\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 70s 70s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6568 - k_mae_mape: 2.2717 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2134 - val_k_mae_mape: 1.5919\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 70s 70s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6568 - k_mae_mape: 2.2717 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2136 - val_k_mae_mape: 1.5920\n",
      "\n",
      "Epoch 00147: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 70s 70s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6567 - k_mae_mape: 2.2716 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2134 - val_k_mae_mape: 1.5919\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 70s 70s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6566 - k_mae_mape: 2.2716 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2134 - val_k_mae_mape: 1.5919\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 70s 70s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6565 - k_mae_mape: 2.2716 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2134 - val_k_mae_mape: 1.5919\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 70s 70s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6566 - k_mae_mape: 2.2716 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2134 - val_k_mae_mape: 1.5919\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 69s 69s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6565 - k_mae_mape: 2.2716 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2134 - val_k_mae_mape: 1.5919\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 70s 70s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6566 - k_mae_mape: 2.2716 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2134 - val_k_mae_mape: 1.5919\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 69s 69s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6566 - k_mae_mape: 2.2716 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2134 - val_k_mae_mape: 1.5919\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 69s 69s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6566 - k_mae_mape: 2.2716 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2134 - val_k_mae_mape: 1.5919\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 69s 69s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6566 - k_mae_mape: 2.2716 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2134 - val_k_mae_mape: 1.5919\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 69s 69s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6566 - k_mae_mape: 2.2716 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2134 - val_k_mae_mape: 1.5919\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 69s 69s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6565 - k_mae_mape: 2.2716 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2134 - val_k_mae_mape: 1.5919\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 70s 70s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6566 - k_mae_mape: 2.2716 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2134 - val_k_mae_mape: 1.5919\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 69s 69s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6566 - k_mae_mape: 2.2716 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2134 - val_k_mae_mape: 1.5919\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 69s 69s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6566 - k_mae_mape: 2.2716 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2134 - val_k_mae_mape: 1.5919\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 69s 69s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6565 - k_mae_mape: 2.2716 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2134 - val_k_mae_mape: 1.5919\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 69s 69s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6565 - k_mae_mape: 2.2716 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2134 - val_k_mae_mape: 1.5919\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 69s 69s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6565 - k_mae_mape: 2.2716 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2134 - val_k_mae_mape: 1.5919\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 69s 69s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6565 - k_mae_mape: 2.2716 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2134 - val_k_mae_mape: 1.5919\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 69s 69s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6565 - k_mae_mape: 2.2716 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2133 - val_k_mae_mape: 1.5919\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 69s 69s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6565 - k_mae_mape: 2.2716 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2134 - val_k_mae_mape: 1.5919\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 69s 69s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6565 - k_mae_mape: 2.2716 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2134 - val_k_mae_mape: 1.5919\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 69s 69s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6565 - k_mae_mape: 2.2716 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2134 - val_k_mae_mape: 1.5919\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 69s 69s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6566 - k_mae_mape: 2.2716 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2133 - val_k_mae_mape: 1.5919\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 69s 69s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6565 - k_mae_mape: 2.2716 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2134 - val_k_mae_mape: 1.5919\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 68s 68s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6565 - k_mae_mape: 2.2716 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2134 - val_k_mae_mape: 1.5919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 173/200\n",
      "1/1 [==============================] - 69s 69s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6565 - k_mae_mape: 2.2716 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2134 - val_k_mae_mape: 1.5919\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 69s 69s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6565 - k_mae_mape: 2.2716 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2134 - val_k_mae_mape: 1.5919\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 69s 69s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6565 - k_mae_mape: 2.2716 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2134 - val_k_mae_mape: 1.5919\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 69s 69s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6566 - k_mae_mape: 2.2716 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2134 - val_k_mae_mape: 1.5919\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 69s 69s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6565 - k_mae_mape: 2.2716 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2133 - val_k_mae_mape: 1.5919\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 69s 69s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6565 - k_mae_mape: 2.2716 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2134 - val_k_mae_mape: 1.5919\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 69s 69s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6565 - k_mae_mape: 2.2716 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2133 - val_k_mae_mape: 1.5919\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 69s 69s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6565 - k_mae_mape: 2.2716 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2134 - val_k_mae_mape: 1.5919\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 69s 69s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6565 - k_mae_mape: 2.2716 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2133 - val_k_mae_mape: 1.5919\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 69s 69s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6565 - k_mae_mape: 2.2716 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2135 - val_k_mae_mape: 1.5919\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 69s 69s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6565 - k_mae_mape: 2.2716 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2133 - val_k_mae_mape: 1.5919\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 69s 69s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6565 - k_mae_mape: 2.2716 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2135 - val_k_mae_mape: 1.5919\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 69s 69s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6565 - k_mae_mape: 2.2716 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2133 - val_k_mae_mape: 1.5919\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 69s 69s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6566 - k_mae_mape: 2.2716 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2135 - val_k_mae_mape: 1.5919\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 69s 69s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6565 - k_mae_mape: 2.2716 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2133 - val_k_mae_mape: 1.5919\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 68s 68s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6565 - k_mae_mape: 2.2716 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2134 - val_k_mae_mape: 1.5919\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 69s 69s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6564 - k_mae_mape: 2.2716 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2133 - val_k_mae_mape: 1.5919\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 69s 69s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6565 - k_mae_mape: 2.2716 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2134 - val_k_mae_mape: 1.5919\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 69s 69s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6565 - k_mae_mape: 2.2716 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2133 - val_k_mae_mape: 1.5919\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 69s 69s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6565 - k_mae_mape: 2.2716 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2134 - val_k_mae_mape: 1.5919\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 69s 69s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6564 - k_mae_mape: 2.2716 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2133 - val_k_mae_mape: 1.5919\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 69s 69s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6565 - k_mae_mape: 2.2716 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2135 - val_k_mae_mape: 1.5919\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 69s 69s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6564 - k_mae_mape: 2.2716 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2133 - val_k_mae_mape: 1.5919\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 68s 68s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6564 - k_mae_mape: 2.2716 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2134 - val_k_mae_mape: 1.5919\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 68s 68s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6564 - k_mae_mape: 2.2716 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2133 - val_k_mae_mape: 1.5919\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 68s 68s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6565 - k_mae_mape: 2.2716 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2135 - val_k_mae_mape: 1.5919\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 68s 68s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6564 - k_mae_mape: 2.2716 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2133 - val_k_mae_mape: 1.5919\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 69s 69s/step - loss: 4.1056 - k_mean_absolute_percentage_error: 52.6564 - k_mae_mape: 2.2716 - val_loss: 2.5288 - val_k_mean_absolute_percentage_error: 57.2134 - val_k_mae_mape: 1.5919\n",
      "num vals: 99440\n",
      "mae: 2.528824\n",
      "mape: 62.14253902435303\n",
      "cmape: 61.049872636795044\n",
      "ae_p75: 3.000042736530304\n",
      "ae_p90: 6.578974723815918\n",
      "ape_p75: 3.000042736530304\n",
      "ape_p90: 6.578974723815918\n",
      "Overall results per week\n",
      "----------------------------------------\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "----------------------------------------\n",
      "Results per target\n",
      "GSW\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "VGAD\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "CUA\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "TIX\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "FLT\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "PTN\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "OVN\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "IRU\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "FDM\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "HDF\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "SVW\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "BWX\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "FPH\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "AGL\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "JHC\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "HIN\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "TRS\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "HGO\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "SGP\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "AVN\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "DOW\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "NVL\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "AOG\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "BGL\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "HSN\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "CSS\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "SWM\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "RIC\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "CMI\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "MKE\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "BCI\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "IBG\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "SWJ\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "IXP\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "MYX\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "RNY\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "CZA\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "SRV\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "VMT\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "CDM\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "CLQ\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "BPS\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "CSE\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "ORR\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "OOK\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "EOS\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "BRL\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "BWP\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "ENC\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "FCT\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "SIO\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "SZG\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "ECG\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "SXA\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "MHC\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "PHK\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "ICT\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "CLH\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "NTM\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "TAS\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "REV\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "CII\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "FND\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "ZGM\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "GOE\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "SSI\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "QMN\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "EAL\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "DSB\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "PNX\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "KRS\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "RXH\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "LAU\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "CNW\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "GSZ\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "IBC\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "SAS\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "EAS\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "OCC\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "AIS\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "INK\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "AIK\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "EVM\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "MSV\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "XST\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "GMR\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "WWI\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "JYC\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "BSM\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "VRX\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "TKL\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "WFE\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "ZMI\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "SHK\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "DGO\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "BD1\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "ALT\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "SES\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "RNO\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "MXC\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "IRC\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "GTR\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "AYM\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "RLC\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "IVG\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "MDG\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "MPE\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "MOT\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "OAR\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "HGL\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "DSE\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "VII\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "KTL\n",
      "Week 1\n",
      "Week 2\n",
      "Week 3\n",
      "Week 4\n",
      "Week 5\n",
      "Week 6\n",
      "Week 7\n",
      "Week 8\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.core import *\n",
    "from keras.layers.recurrent import LSTM, GRU, SimpleRNN, RNN\n",
    "from keras.layers import Bidirectional, Flatten, RepeatVector, TimeDistributed, Concatenate\n",
    "from keras.regularizers import L1L2\n",
    "from keras.models import *\n",
    "from keras import optimizers\n",
    "from keras.callbacks import EarlyStopping, CSVLogger, ModelCheckpoint, ReduceLROnPlateau\n",
    "from AdamW import AdamW\n",
    "from stats_operations import *\n",
    "from attention import AttentionLayer\n",
    "\n",
    "# rnn_train_y = rnn_train_y.reshape((rnn_train_y.shape[0], n_outputs))\n",
    "# rnn_test_y = rnn_test_y.reshape((rnn_test_y.shape[0], n_outputs))\n",
    "\n",
    "encoder_layers = [2, 2, 1, 1]\n",
    "decoder_layers = [1, 1, 2, 2]\n",
    "batch_size = 128\n",
    "num_epochs = 200\n",
    "num_samples = rnn_flat_train_x.shape[0]\n",
    "n_timesteps = rnn_flat_train_x.shape[1]\n",
    "n_features = rnn_flat_train_x.shape[2]\n",
    "n_output_timesteps = rnn_flat_train_y.shape[1]\n",
    "n_output_features = rnn_flat_train_y.shape[2]\n",
    "steps_per_epoch = int(num_samples / batch_size)\n",
    "\n",
    "scaled_encoder_layers = []\n",
    "for layer in encoder_layers:\n",
    "    scaled_encoder_layers.append(max(int(n_features * layer), 1))\n",
    "\n",
    "scaled_decoder_layers = []\n",
    "for layer in decoder_layers:\n",
    "    scaled_decoder_layers.append(max(int(n_features * layer), 1))\n",
    "\n",
    "print('encoder layers', scaled_encoder_layers)\n",
    "print('decoder layers', scaled_decoder_layers)\n",
    "print('batch_size', batch_size)\n",
    "print('epochs', num_epochs)\n",
    "print('Train shape')\n",
    "print(num_samples, ',', n_timesteps, ',', n_features)\n",
    "print(rnn_flat_train_x.shape)\n",
    "print('Test shape')\n",
    "print(num_samples, ',', n_output_timesteps, ', ', n_output_features)\n",
    "print(rnn_flat_train_y.shape)\n",
    "print('input steps', n_timesteps)\n",
    "print('input features', n_features)\n",
    "print('output steps', n_output_timesteps)\n",
    "print('output features', n_output_features)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30)\n",
    "checkpointer = ModelCheckpoint(\n",
    "filepath='./weights/nn-weights.hdf5', verbose=0, save_best_only=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=15, min_lr=1e-5, verbose=1)\n",
    "weight_decay = 0.005 * batch_size / num_samples / num_epochs **0.5\n",
    "\n",
    "encoder_inputs = Input(shape=(n_timesteps, n_features))\n",
    "input_layer = GRU(scaled_encoder_layers[0], activation='relu')(encoder_inputs)\n",
    "repeated_input = RepeatVector(n_outputs)(input_layer)\n",
    "layer_2 = GRU(scaled_encoder_layers[1], activation='relu', return_sequences=True)(repeated_input)\n",
    "layer_2_time_distributed = TimeDistributed(Dense(scaled_encoder_layers[1], activation='relu'))(layer_2)\n",
    "# layer_3 = GRU(scaled_encoder_layers[2], activation='relu', return_sequences=True)(layer_2_time_distributed)\n",
    "# layer_3_time_distributed = TimeDistributed(Dense(scaled_encoder_layers[2], activation='relu'))(layer_3)\n",
    "final_encoder_layer, encoder_state = GRU(scaled_encoder_layers[3], activation='relu', return_sequences=True, return_state=True)(layer_2_time_distributed)\n",
    "\n",
    "decoder_inputs = Input(shape=(n_output_timesteps, n_output_features))\n",
    "decoder_layer_1, decoder_state = GRU(scaled_decoder_layers[0], \n",
    "                                     activation='relu', \n",
    "                                     return_sequences=True, \n",
    "                                     return_state=True)(decoder_inputs, initial_state=encoder_state)\n",
    "decoder_layer_1_time_distributed = TimeDistributed(Dense(scaled_decoder_layers[0], activation='relu'))(decoder_layer_1)\n",
    "decoder_layer_2 = GRU(scaled_decoder_layers[1], activation='relu', return_sequences=True)(decoder_layer_1_time_distributed)\n",
    "decoder_layer_2_time_distributed = TimeDistributed(Dense(scaled_decoder_layers[1], activation='relu'))(decoder_layer_2)\n",
    "decoder_layer_3 = GRU(scaled_decoder_layers[2], activation='relu', return_sequences=True)(decoder_layer_2_time_distributed)\n",
    "decoder_layer_3_time_distributed = TimeDistributed(Dense(scaled_decoder_layers[2], activation='relu'))(decoder_layer_3)\n",
    "# decoder_layer_4 = GRU(scaled_decoder_layers[3], activation='relu', return_sequences=True)(decoder_layer_3_time_distributed)\n",
    "# decoder_layer_4_time_distributed = TimeDistributed(Dense(scaled_decoder_layers[3], activation='relu'))(decoder_layer_4)\n",
    "decoder_output_layer = TimeDistributed(Dense(n_output_features))(decoder_layer_3_time_distributed)\n",
    "\n",
    "model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_output_layer)\n",
    "model.compile(loss='mae', optimizer=optimizers.RMSprop(lr=0.002), metrics=[k_mean_absolute_percentage_error, k_mae_mape])\n",
    "print(model.summary())\n",
    "                            \n",
    "decoder_train_input = K.zeros(rnn_flat_train_y.shape)\n",
    "decoder_test_input = K.zeros(rnn_flat_test_y.shape)\n",
    "    \n",
    "# fit network\n",
    "model.fit([rnn_flat_train_x, decoder_train_input], rnn_flat_train_y,\n",
    "           validation_data = ([rnn_flat_test_x, decoder_test_input], rnn_flat_test_y),\n",
    "           epochs=num_epochs, \n",
    "           steps_per_epoch=1, \n",
    "           validation_steps=1,\n",
    "           callbacks=[early_stopping, checkpointer, reduce_lr],\n",
    "           verbose=1)\n",
    "\n",
    "# print(encoder_state.shape)\n",
    "# print(encoder_state)\n",
    "\n",
    "model.load_weights('./weights/nn-weights.hdf5')\n",
    "y_flat_test_predictions = model.predict([rnn_flat_test_x, decoder_test_input], steps=1)\n",
    "\n",
    "y_true = rnn_flat_test_y.flatten()\n",
    "y_pred = y_flat_test_predictions.flatten()    \n",
    "mae, mape, cmape, smape, ae_p25, ae_p50, ae_p75, ae_p90, rsquared, \\\n",
    "        explain_variance, ape_p75, ape_p90, num_vals = eval_results(y_true, y_pred)\n",
    "print('num vals:', num_vals)\n",
    "print('mae:', mae)\n",
    "print('mape:', mape)\n",
    "print('cmape:', cmape)\n",
    "print('ae_p75:', ae_p75)\n",
    "print('ae_p90:', ae_p90)\n",
    "print('ape_p75:', ae_p75)\n",
    "print('ape_p90:', ae_p90)\n",
    "\n",
    "df_results = eval_flat_predictions(rnn_flat_test_y, y_flat_test_predictions, target_col_names)\n",
    "df_results.to_csv('./results/seq2seq.csv')\n",
    "\n",
    "print('Done')\n",
    "\n",
    "#2.5288 3 layer\n",
    "#2.5293 2 layer\n",
    "#2.5288 4 layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = eval_flat_predictions(rnn_flat_test_y, y_flat_test_predictions, target_col_names)\n",
    "df_results.to_csv('./results/seq2seq.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import LSTM, GRU\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.regularizers import L1L2\n",
    "from keras.callbacks import EarlyStopping, CSVLogger, ModelCheckpoint, ReduceLROnPlateau\n",
    "from AdamW import AdamW\n",
    "\n",
    "rnn_train_y = rnn_train_y.reshape(len(rnn_train_y), 4)\n",
    "rnn_test_y = rnn_test_y.reshape(len(rnn_test_y), 4)\n",
    "\n",
    "layers = [2, 2, 1]\n",
    "batch_size = 128\n",
    "num_epochs = 50\n",
    "num_samples = rnn_train_x.shape[0]\n",
    "n_timesteps =  rnn_train_x.shape[1]\n",
    "n_features = rnn_train_x.shape[2]\n",
    "n_outputs = rnn_train_y.shape[1]\n",
    "\n",
    "scaled_layers = []\n",
    "for layer in layers:\n",
    "    scaled_layers.append(max(int(n_features * layer), 1))\n",
    "\n",
    "    \n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30)\n",
    "# csv_logger = CSVLogger('./logs/nn-training-log.log')\n",
    "checkpointer = ModelCheckpoint(\n",
    "filepath='./weights/nn-weights.hdf5', verbose=0, save_best_only=True)\n",
    "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=15, min_lr=1e-5)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=15, min_lr=1e-5, verbose=1)\n",
    "weight_decay = 0.005 * batch_size / num_samples / num_epochs **0.5\n",
    "\n",
    "print(scaled_layers)\n",
    "print(batch_size)\n",
    "print(num_epochs)\n",
    "print(num_samples)\n",
    "print(n_timesteps)\n",
    "print(n_features)\n",
    "print(n_outputs)\n",
    "print(weight_decay)\n",
    "\n",
    "\n",
    "# define model\n",
    "model2 = Sequential()\n",
    "model2.add(LSTM(scaled_layers[0], activation='relu', \n",
    "                input_shape=(n_timesteps, n_features), \n",
    "                kernel_regularizer=L1L2(0.001, 0.001),\n",
    "                return_sequences=True))\n",
    "# model2.add(LSTM(scaled_layers[1], activation='relu', return_sequences=True))\n",
    "model2.add(Dense(scaled_layers[2], activation='relu'))\n",
    "model2.add(Dense(n_outputs))\n",
    "# model2.compile(loss='mae', optimizer='adam')\n",
    "model2.compile(loss='mean_squared_logarithmic_error', optimizer=AdamW(weight_decay=weight_decay))\n",
    "\n",
    "# fit network\n",
    "model2.fit(rnn_train_x, \n",
    "          rnn_train_y,\n",
    "#           validation_split=0.2,\n",
    "          validation_data = (rnn_test_x, rnn_test_y),\n",
    "          epochs=num_epochs, \n",
    "          batch_size=batch_size, \n",
    "          callbacks=[early_stopping, checkpointer, reduce_lr],\n",
    "          verbose=1)\n",
    "\n",
    "model2.load_weights('./weights/nn-weights.hdf5')\n",
    "y_test_predictions = model2.predict(rnn_test_x)\n",
    "\n",
    "model_eval = evaluate_results(rnn_test_symbols, y_test_predictions, rnn_test_y)\n",
    "model_eval.to_csv('./results/lstm-%d-%d-%d-epochs-adamw-l1l2-001.csv'%(n_input, n_out, num_epochs))\n",
    "print('Done')\n",
    "model_eval\n",
    "\n",
    "# 12-4, 100 epochs, L1L2 0.01: loss: 7.9516 - val_loss: 5.8052\n",
    "# 12-4, 200 epochs, L1L2 0.01: loss: 7.8844 - val_loss: 5.7364\n",
    "# 16-4, 200 epochs, L1L2 0.01: loss: 7.9167 - val_loss: 5.5488\n",
    "# 20-4, 200 epochs, L1L2 0.01: loss: 8.3992 - val_loss: 6.1353\n",
    "# 8-4, 200 epochs, L1L2 0.01: loss: 7.9486 - val_loss: 5.7188\n",
    "# 8-4, 200 epochs, L1L2 0.008: loss: 7.9347 - val_loss: 5.7033\n",
    "# 8-4, 200 epochs, L1L2 0.005: loss: 7.9348 - val_loss: 5.7030\n",
    "# 16-4, 200 epochs, L1L2 0.001, AdamW: loss: 7.9842 - val_loss: 5.7384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Executing keras predictions...')\n",
    "print('Data: rnn_test_x', rnn_test_x.shape)\n",
    "print('Data: rnn_test_y', rnn_test_y.shape)\n",
    "print('Data: rnn_test_symbols', rnn_test_symbols.shape)\n",
    "\n",
    "y_test_predictions = model2.predict(rnn_test_x)\n",
    "print('Predictions: ', y_test_predictions.shape)\n",
    "\n",
    "\n",
    "model_eval = evaluate_results(rnn_test_symbols, y_test_predictions, rnn_test_y)\n",
    "model_eval.to_csv('./results/lstm-16-4-200-epochs.csv')\n",
    "print('Done')\n",
    "model_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Time Distributed LSTM\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.regularizers import L1L2\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, CSVLogger, ModelCheckpoint, ReduceLROnPlateau\n",
    "from AdamW import AdamW\n",
    "\n",
    "rnn_train_y = rnn_train_y.reshape((rnn_train_y.shape[0], 4, 1))\n",
    "rnn_test_y = rnn_test_y.reshape((rnn_test_y.shape[0], 4, 1))\n",
    "\n",
    "layers = [2, 1, 1]\n",
    "batch_size = 128\n",
    "num_epochs = 200\n",
    "num_samples = rnn_train_x.shape[0]\n",
    "\n",
    "scaled_layers = []\n",
    "for layer in layers:\n",
    "    scaled_layers.append(max(int(n_features * layer), 1))\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30)\n",
    "# csv_logger = CSVLogger('./logs/nn-training-log.log')\n",
    "checkpointer = ModelCheckpoint(\n",
    "filepath='./weights/nn-weights.hdf5', verbose=0, save_best_only=True)\n",
    "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=15, min_lr=1e-5)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=15, min_lr=1e-5, verbose=1)\n",
    "weight_decay = 0.005 * batch_size / num_samples / num_epochs **0.5\n",
    "\n",
    "print(scaled_layers)\n",
    "print(batch_size)\n",
    "print(num_epochs)\n",
    "print(num_samples)\n",
    "print(weight_decay)\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=128, kernel_size=4, activation='relu', \n",
    "                 input_shape=(n_timesteps,n_features)\n",
    "                ))\n",
    "#                ,   kernel_regularizer=L1L2(0.0001, 0.0001)))\n",
    "model.add(Conv1D(filters=128, kernel_size=4, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(RepeatVector(n_outputs))\n",
    "\n",
    "# for layer_size in scaled_layers[1:-1]:\n",
    "model.add(LSTM(scaled_layers[1], activation='relu', return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(scaled_layers[2], activation='relu')))\n",
    "model.add(TimeDistributed(Dense(1)))\n",
    "\n",
    "model.compile(loss='mae', optimizer=Adam(lr=0.1))\n",
    "# fit network\n",
    "model.fit(rnn_train_x, \n",
    "          rnn_train_y,\n",
    "          validation_data = (rnn_test_x, rnn_test_y),\n",
    "          epochs=num_epochs, \n",
    "          batch_size=batch_size, \n",
    "          callbacks=[early_stopping, checkpointer, reduce_lr],\n",
    "          verbose=1)\n",
    "\n",
    "model.load_weights('./weights/nn-weights.hdf5')\n",
    "y_test_predictions = model.predict(rnn_test_x)\n",
    "\n",
    "model_eval = evaluate_results(rnn_test_symbols, y_test_predictions, rnn_test_y)\n",
    "model_eval.to_csv('./results/cnn-lstm-%d-%d-%d-epochs.csv' %(n_input, n_out, num_epochs))\n",
    "\n",
    "print('Done')\n",
    "model_eval\n",
    "\n",
    "# 16-4, 200 epochs: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Time Distributed LSTM\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import ConvLSTM2D\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.regularizers import L1L2\n",
    "from keras.callbacks import EarlyStopping, CSVLogger, ModelCheckpoint, ReduceLROnPlateau\n",
    "from AdamW import AdamW\n",
    "\n",
    "# n_input = n_length * n_steps\n",
    "n_length = 4\n",
    "n_steps = 4\n",
    "        \n",
    "conv_train_x = rnn_train_x.reshape((rnn_train_x.shape[0], n_steps, 1, n_length, n_features))\n",
    "conv_test_x = rnn_test_x.reshape((rnn_test_x.shape[0], n_steps, 1, n_length, n_features))\n",
    "\n",
    "conv_train_y = rnn_train_y.reshape((rnn_train_y.shape[0], 4, 1))\n",
    "conv_test_y = rnn_test_y.reshape((rnn_test_y.shape[0], 4, 1))\n",
    "\n",
    "layers = [2, 1, 1]\n",
    "batch_size = 128\n",
    "num_epochs = 200\n",
    "num_samples = rnn_train_x.shape[0]\n",
    "\n",
    "scaled_layers = []\n",
    "for layer in layers:\n",
    "    scaled_layers.append(max(int(n_features * layer), 1))\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30)\n",
    "# csv_logger = CSVLogger('./logs/nn-training-log.log')\n",
    "checkpointer = ModelCheckpoint(\n",
    "filepath='./weights/nn-weights.hdf5', verbose=0, save_best_only=True)\n",
    "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=15, min_lr=1e-5)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=15, min_lr=1e-5, verbose=1)\n",
    "weight_decay = 0.005 * batch_size / num_samples / num_epochs **0.5\n",
    "\n",
    "print(scaled_layers)\n",
    "print(batch_size)\n",
    "print(num_epochs)\n",
    "print(num_samples)\n",
    "print(weight_decay)\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(ConvLSTM2D(256, (1,4), activation='relu', input_shape=(n_steps, 1, n_length, n_features)))\n",
    "model.add(Flatten())\n",
    "model.add(RepeatVector(n_outputs))\n",
    "    \n",
    "# for layer_size in scaled_layers[1:-1]:\n",
    "model.add(GRU(scaled_layers[0], activation='relu', return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(scaled_layers[1], activation='relu')))\n",
    "model.add(TimeDistributed(Dense(1)))\n",
    "\n",
    "model.compile(loss='mae', optimizer=optimizers.RMSprop())\n",
    "# fit network\n",
    "model.fit(conv_train_x, \n",
    "          conv_train_y,\n",
    "          validation_data = (conv_test_x, conv_test_y),\n",
    "          epochs=num_epochs, \n",
    "          batch_size=batch_size, \n",
    "          callbacks=[early_stopping, checkpointer, reduce_lr],\n",
    "          verbose=1)\n",
    "\n",
    "model.load_weights('./weights/nn-weights.hdf5')\n",
    "y_test_predictions = model.predict(conv_test_x)\n",
    "\n",
    "model_eval = evaluate_results(rnn_test_symbols, y_test_predictions, rnn_test_y)\n",
    "model_eval.to_csv('./results/convlstm2d-%d-%d-%d-epochs.csv' %(n_input, n_out, num_epochs))\n",
    "\n",
    "print('Done')\n",
    "model_eval\n",
    "\n",
    "# 16-4, 200 epochs: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepTimeSeries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DeepTimeSeries.models.RNN2Dense import * \n",
    "from DeepTimeSeries.models.Seq2Seq import *\n",
    "from keras import optimizers # RMSProp, Adagrad, Adadelta, Adamax, Nadam, Adam\n",
    "from keras.regularizers import L1L2\n",
    "from AdamW import AdamW\n",
    "\n",
    "# hyperparameters =======================\n",
    "n_memory_steps = 16 # time steps for encoder\n",
    "n_forcast_steps = 8 # time steps for decoder\n",
    "batch_size = 128 # batch size for training\n",
    "epochs = 500 # epochs for training\n",
    "test_model = 'Seq2Seq_2'    # 'RNN2Dense_1' / 'Seq2Seq_1' / 'Seq2Seq_2'\n",
    "cell = 'GRU'           # 'SimpleRNN' / 'LSTM' / 'GRU'\n",
    "# =======================================\n",
    "\n",
    "rnn_train_y = rnn_train_y.reshape((rnn_train_y.shape[0], 4, 1))\n",
    "rnn_test_y = rnn_test_y.reshape((rnn_test_y.shape[0], 4, 1))\n",
    "\n",
    "print(rnn_train_x.shape)\n",
    "print(rnn_train_y.shape)\n",
    "\n",
    "# build model\n",
    "if test_model == 'RNN2Dense_1':\n",
    "    dts = RNN2Dense_1(rnn_train_x.shape[1:], rnn_train_y.shape[1:], cell, 300, (128,3))\n",
    "elif test_model == 'RNN2Dense_2':\n",
    "    dts = RNN2Dense_2(rnn_train_x.shape[1:], rnn_train_y.shape[1:], cell, 300, (128,3))\n",
    "elif test_model == 'Seq2Seq_1':\n",
    "    dts = Seq2Seq_1(rnn_train_x.shape[1:], rnn_train_y.shape[1:], cell, 300)\n",
    "elif test_model == 'Seq2Seq_2':\n",
    "    dts = Seq2Seq_2(rnn_train_x.shape[1:], rnn_train_y.shape[1:], cell, 500, False, None, 0., 0.) # L1L2(0.001, 0.001)\n",
    "print(dts.summary())\n",
    "# compile model\n",
    "dts.compile(optimizer=optimizers.Nadam(), loss='mape', metrics='mae')\n",
    "\n",
    "# train model\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30)\n",
    "checkpointer = ModelCheckpoint(\n",
    "filepath='./weights/nn-weights.hdf5', verbose=0, save_best_only=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=15, min_lr=1e-5, verbose=1)\n",
    "\n",
    "dts.fit(rnn_train_x, \n",
    "              rnn_train_y, \n",
    "              validation_data=(rnn_test_x, rnn_test_y),\n",
    "              batch_size=batch_size,   \n",
    "              epochs=epochs, \n",
    "              callbacks=[early_stopping, checkpointer, reduce_lr],\n",
    "              verbose=1)\n",
    "\n",
    "y_test_predictions = dts.predict(rnn_test_x)\n",
    "\n",
    "model_eval = evaluate_results(rnn_test_symbols, y_test_predictions, rnn_test_y)\n",
    "model_eval.to_csv('./results/%s-%s-%d-%d-%d-epochs.csv' %(test_model, cell, n_input, n_out, num_epochs))\n",
    "\n",
    "print('Done')\n",
    "model_eval\n",
    "\n",
    "# Seq2Seq1 LSTM: loss: 1.6082 - val_loss: 1.3650\n",
    "# Seq2Seq1 GRU: loss: 0.7289 - val_loss: 1.1589\n",
    "# Seq2Seq1 SimpeRNN: loss: 0.7611 - val_loss: 1.1223\n",
    "# Seq2Seq2 LSTM: loss: 0.9287 - val_loss: 1.0096\n",
    "# Seq2Seq2 GRU: loss: 0.5922 - val_loss: 1.3541 **\n",
    "# Seq2Seq2 GRU L1L2: loss: 0.8083 - val_loss: 1.3500\n",
    "# Seq2Seq2 GRU RMSProp: loss: 0.3581 - val_loss: 1.4363\n",
    "# Seq2Seq2 GRU RMSProp L1L2 0.001: loss: 0.8680 - val_loss: 1.3121\n",
    "# Seq2Seq2 GRU Adagrad: loss: 1.6082 - val_loss: 1.3650\n",
    "# Seq2Seq2 GRU Adadelta: loss: 0.9741 - val_loss: 1.0678\n",
    "# Seq2Seq2 GRU Adamax: loss: 0.6380 - val_loss: 1.3190\n",
    "# Seq2Seq2 GRU Nadam: loss: 0.6843 - val_loss: 1.3261\n",
    "# Seq2Seq2 SimpeRNN: loss: 0.7180 - val_loss: 1.2733\n",
    "# RNN2Dense_1 LSTM: loss: 1.0986 - val_loss: 0.9767\n",
    "# RNN2Dense_1 GRU: loss: 1.1712 - val_loss: 1.1004\n",
    "# RNN2Dense_1 SimpeRNN: loss: 1.2591 - val_loss: 1.0768\n",
    "\n",
    "# Seq2Seq2 200 epochs GRU L1L2 0.001: mae: 5.704368591308594 mape: 72.01702147722244 medae: 2.7182743549346924\n",
    "# Seq2Seq2 500 epochs GRU L1L2 0.001:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "K = keras.backend\n",
    "\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.layers import RepeatVector, Concatenate, Activation\n",
    "from tensorflow.python.keras.layers import Reshape, Input, Dense, Dot, LSTM\n",
    "from tensorflow.python.keras import regularizers\n",
    "\n",
    "from tensorflow.python.keras.models import load_model as keras_load_model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "\n",
    "\n",
    "# Sometimes helpful to implement own softmax activation function to\n",
    "# better manage calculations along specific axes.\n",
    "def softmax_activation(x):\n",
    "    e = K.exp(x - K.max(x, axis=1, keepdims=True))\n",
    "    s = K.sum(e, axis=1, keepdims=True)\n",
    "    return e / s\n",
    "\n",
    "\n",
    "class AttentionModel(object):\n",
    "\n",
    "    def __init__(self, x, y,\n",
    "                 layer_1_rnn_units,\n",
    "                 attn_dense_nodes=0,\n",
    "                 epochs=100,\n",
    "                 batch_size=128,\n",
    "                 shared_attention_layer=True,\n",
    "                 chg_yield=False,\n",
    "                 float_type='float32',\n",
    "                 regularization=(0.00001, '00001'),\n",
    "                 window=52,\n",
    "                 predict=1):\n",
    "        K.clear_session()\n",
    "        tf.reset_default_graph()\n",
    "        self.set_learning(True)\n",
    "\n",
    "        # Scientific computing uses 'float64' but\n",
    "        # machine learning works much faster with 'float32'.\n",
    "        self.float_type = float_type\n",
    "        K.set_floatx(self.float_type)\n",
    "\n",
    "        # Capture inputs to instance variables.\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.shared_attention_layer = shared_attention_layer\n",
    "\n",
    "        self.layer_1_rnn_units = layer_1_rnn_units\n",
    "        self.layer_2_rnn_units = self.layer_1_rnn_units\n",
    "        self.attn_dense_nodes = attn_dense_nodes\n",
    "\n",
    "        self.num_obs = self.x.shape[0]\n",
    "        self.input_len = self.x.shape[1]\n",
    "        self.input_dims = self.x.shape[2]\n",
    "        self.num_outputs = self.y.shape[1]\n",
    "\n",
    "        self.regularization = regularization[0]\n",
    "\n",
    "        assert self.x.shape[0] == self.y.shape[0]\n",
    "\n",
    "        # Set the directory structure.\n",
    "        self.model_dir = f'models//window_{window}_predict_{predict}//'\n",
    "\n",
    "        self.model_name = f'{\"yield_changes\" if chg_yield==True else \"yield_levels\"}//' \\\n",
    "                          f'model_{layer_1_rnn_units}_rnn_{attn_dense_nodes}_dense_attn_' \\\n",
    "                          f'{epochs}_epochs_' \\\n",
    "                          f'{batch_size}_batch_' \\\n",
    "                          f'{\"shared_attention\" if shared_attention_layer else\"\"}_' \\\n",
    "                          f'{\"change_yield\" if chg_yield else \"level_yield\"}_' \\\n",
    "                          f'{regularization[1]}_reg'\n",
    "\n",
    "        # Activation function for the attention mechanism dense layer(s).\n",
    "        self.attn_dense_activation = 'selu'\n",
    "        self.attn_dense_initializer = 'lecun_normal'\n",
    "        \n",
    "    def delete_model(self):\n",
    "        try:\n",
    "            os.remove(f'{self.model_dir}{self.model_name}.h5')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    def load_model(self):\n",
    "        try:\n",
    "            self.model = keras_load_model(f'{self.model_dir}{self.model_name}.h5',\n",
    "                                          custom_objects={'softmax_activation': softmax_activation})\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def save_model(self):\n",
    "        try:\n",
    "            self.model.save(f'{self.model_dir}{self.model_name}.h5')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def set_learning(self, learning):\n",
    "        if learning:\n",
    "            self.is_learning_phase = 1\n",
    "            K.set_learning_phase(self.is_learning_phase)\n",
    "            tf.keras.backend.set_learning_phase(True)\n",
    "        else:\n",
    "            self.is_learning_phase = 0\n",
    "            K.set_learning_phase(self.is_learning_phase)\n",
    "            tf.keras.backend.set_learning_phase(False)\n",
    "\n",
    "    # Method that constructs shared layers. A shared layer means its learned parameters\n",
    "    # are the same no matter where the layer is used in the neural network.\n",
    "    def make_shared_layers(self):\n",
    "        if self.regularization > 0.:\n",
    "            self.kernel_reg = regularizers.l2(self.regularization)\n",
    "            self.bias_reg = regularizers.l2(self.regularization)\n",
    "            self.recurrent_reg = regularizers.l2(self.regularization)\n",
    "            self.recurrent_dropout = 0.1\n",
    "        else:\n",
    "            self.kernel_reg = self.bias_reg = self.recurrent_reg = None\n",
    "            self.recurrent_dropout = 0.0\n",
    "\n",
    "        if self.shared_attention_layer:\n",
    "            # This is an optional intermediate dense layer in the attention network.\n",
    "            # If it is not present, the attention mechanism goes straight from inputs to weights.\n",
    "            if self.attn_dense_nodes > 0:\n",
    "                self.attn_middle_dense_layer = Dense(self.attn_dense_nodes,\n",
    "                                                     kernel_regularizer=self.kernel_reg,\n",
    "                                                     bias_regularizer=self.bias_reg,\n",
    "                                                     activation=self.attn_dense_activation,\n",
    "                                                     kernel_initializer=self.attn_dense_initializer,\n",
    "                                                     name='attention_mid_dense_shared')\n",
    "\n",
    "            # This is the layer in the attention mechanism that gives the attention weights.\n",
    "            self.attention_final_dense_layer = Dense(1,\n",
    "                                                     kernel_regularizer=self.kernel_reg,\n",
    "                                                     bias_regularizer=self.bias_reg,\n",
    "                                                     activation=self.attn_dense_activation,\n",
    "                                                     kernel_initializer=self.attn_dense_initializer,\n",
    "                                                     name='attention_final_dense_shared')\n",
    "\n",
    "        # Output-level LSTM cell.\n",
    "        self.layer_2_LSTM_cell = LSTM(self.layer_2_rnn_units,\n",
    "                                      kernel_regularizer=self.kernel_reg,\n",
    "                                      recurrent_regularizer=self.recurrent_reg,\n",
    "                                      bias_regularizer=self.bias_reg,\n",
    "                                      recurrent_dropout=self.recurrent_dropout,\n",
    "                                      return_state=True,\n",
    "                                      name='layer_2_LSTM')\n",
    "\n",
    "        # Final output (i.e., the prediction).\n",
    "        self.dense_output = Dense(1,\n",
    "                                  kernel_regularizer=self.kernel_reg,\n",
    "                                  bias_regularizer=self.bias_reg,\n",
    "                                  activation='linear',\n",
    "                                  name='dense_output')\n",
    "\n",
    "    # Builds the neural network. An LSTM+attention model doesn't need this much code.\n",
    "    # This method is long because it sets lots of layer parameters and because\n",
    "    # it handles four contingencies: (1) whether the attention mechanism is\n",
    "    # always the same or is different for every prediction node, and (2) whether or\n",
    "    # not the attention mechanism has an intermediate dense layer.\n",
    "    def build_attention_rnn(self):\n",
    "        self.make_shared_layers()\n",
    "\n",
    "        inputs = Input(shape=(self.input_len, self.input_dims), dtype=self.float_type)\n",
    "\n",
    "        X = LSTM(self.layer_1_rnn_units,\n",
    "                 kernel_regularizer=self.kernel_reg,\n",
    "                 recurrent_regularizer=self.recurrent_reg,\n",
    "                 bias_regularizer=self.bias_reg,\n",
    "                 recurrent_dropout=self.recurrent_dropout,\n",
    "                 return_sequences=True)(inputs)\n",
    "\n",
    "        X = Reshape((self.input_len, self.layer_2_rnn_units))(X)\n",
    "\n",
    "        h_start = Input(shape=(self.layer_2_rnn_units,), name='h_start')\n",
    "        c_start = Input(shape=(self.layer_2_rnn_units,), name='c_start')\n",
    "        h_prev = h_start\n",
    "        c_prev = c_start\n",
    "\n",
    "        outputs = list()\n",
    "\n",
    "        # This section constructs the attention mechanism and the output-level LSTM\n",
    "        # layer that leads to the predictions.\n",
    "        #\n",
    "        # There is an extra LSTM cell that is not attached to any prediction but\n",
    "        # which begins the output-level RNN sequence. This avoids sending in a bunch\n",
    "        # of zero values to the first usage of the attention mechanism.\n",
    "        #\n",
    "        # One way to avoid this extra LSTM cell might be to set the LSTM intial state\n",
    "        # tensors \"h_start\" and \"c_start\" as trainable (instead of zeros).\n",
    "        for t in range(self.num_outputs + 1):\n",
    "            h_prev_repeat = RepeatVector(self.input_len)(h_prev)\n",
    "            joined = Concatenate(axis=-1)([X, h_prev_repeat])\n",
    "\n",
    "            if self.attn_dense_nodes > 0:\n",
    "                if self.shared_attention_layer:\n",
    "                    joined = self.attn_middle_dense_layer(joined)\n",
    "                else:\n",
    "                    joined = Dense(self.attn_dense_nodes,\n",
    "                                   kernel_regularizer=self.kernel_reg,\n",
    "                                   bias_regularizer=self.bias_reg,\n",
    "                                   activation=self.attn_dense_activation,\n",
    "                                   kernel_initializer=self.attn_dense_initializer,\n",
    "                                   name=f'attention_mid_dense_{t}')(joined)\n",
    "\n",
    "            if self.shared_attention_layer:\n",
    "                e_vals = self.attention_final_dense_layer(joined)\n",
    "            else:\n",
    "                e_vals = Dense(1,\n",
    "                               kernel_regularizer=self.kernel_reg,\n",
    "                               bias_regularizer=self.bias_reg,\n",
    "                               activation=self.attn_dense_activation,\n",
    "                               kernel_initializer=self.attn_dense_initializer,\n",
    "                               name=f'attention_final_dense_{t}')(joined)\n",
    "\n",
    "            alphas = Activation(softmax_activation, name=f'attention_softmax_{t}')(e_vals)\n",
    "            attentions = Dot(axes=1)([alphas, X])\n",
    "\n",
    "            h_prev, _, c_prev = self.layer_2_LSTM_cell(attentions, initial_state=[h_prev, c_prev])\n",
    "\n",
    "            if t > 0:\n",
    "                out = self.dense_output(h_prev)\n",
    "                outputs.append(out)\n",
    "\n",
    "        self.model = Model(inputs=[inputs, h_start, c_start], outputs=outputs)\n",
    "        self.model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n",
    "\n",
    "        print(self.model.summary())\n",
    "\n",
    "    def fit_model(self):\n",
    "        self.set_learning(True)\n",
    "\n",
    "        h_start = np.zeros((self.num_obs, self.layer_2_rnn_units))\n",
    "        c_start = np.zeros((self.num_obs, self.layer_2_rnn_units))\n",
    "\n",
    "        y_split = np.split(self.y, indices_or_sections=self.num_outputs, axis=1)\n",
    "\n",
    "        self.model.fit([self.x, h_start, c_start],\n",
    "                       y_split,\n",
    "                       epochs=self.epochs,\n",
    "                       batch_size=self.batch_size,\n",
    "                       shuffle=True,\n",
    "                       verbose=2,\n",
    "                       validation_split=0.1)\n",
    "\n",
    "    def calculate_attentions(self, x_data):\n",
    "        self.set_learning(False)\n",
    "\n",
    "        softmax_layer_names = [f'attention_softmax_{t}' for t in range(self.num_outputs + 1)]\n",
    "        softmax_layers = list()\n",
    "\n",
    "        for i, layer_name in enumerate(softmax_layer_names):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            intermediate_layer = Model(inputs=self.model.input,\n",
    "                                       outputs=self.model.get_layer(layer_name).output)\n",
    "            softmax_layers.append(intermediate_layer)\n",
    "\n",
    "        num_obs = x_data.shape[0]\n",
    "        attention_map = np.zeros((num_obs, self.num_outputs, self.input_len))\n",
    "\n",
    "        h_start = np.zeros((1, self.layer_2_rnn_units))\n",
    "        c_start = np.zeros((1, self.layer_2_rnn_units))\n",
    "\n",
    "        for t in range(num_obs):\n",
    "            print(t)\n",
    "            for l_num, layer in enumerate(softmax_layers):\n",
    "                softmax_results = layer.predict([np.expand_dims(x_data[t], axis=0),\n",
    "                                                 h_start,\n",
    "                                                 c_start])\n",
    "                softmax_results = softmax_results[0, :, 0]\n",
    "                attention_map[t, l_num, :] = softmax_results\n",
    "\n",
    "        return attention_map\n",
    "\n",
    "    def heatmap(self, data, title_supplement=None):\n",
    "        plt.rcParams['axes.labelweight'] = 'bold'\n",
    "        plt.rcParams['axes.labelsize'] = 22\n",
    "        plt.rcParams['axes.titlesize'] = 22\n",
    "        plt.rcParams['axes.titleweight'] = 'bold'\n",
    "        plt.rcParams['xtick.labelsize'] = 18\n",
    "        plt.rcParams['ytick.labelsize'] = 18\n",
    "        plt.rcParams['axes.titlepad'] = 12\n",
    "        plt.rcParams['axes.edgecolor'] = '#000000'  # '#FD5E0F'\n",
    "\n",
    "        # Other common color schemes: 'viridis'  'plasma'  'gnuplot'\n",
    "        color_map = 'inferno'\n",
    "        pylab.pcolor(data, cmap=color_map, vmin=0.)\n",
    "        pylab.colorbar()\n",
    "\n",
    "        num_predictions = data.shape[0]\n",
    "        num_timesteps = data.shape[1]\n",
    "\n",
    "        if num_predictions == 4:\n",
    "            pylab.yticks([0.5, 1.5, 2.5, 3.5], ['t+1', 't+2', 't+3', 't+4'])\n",
    "            pylab.ylabel('y: t+1 to t+4')\n",
    "\n",
    "            plt.axhline(y=1., xmin=0.0, xmax=51.0, linewidth=1, color='w')\n",
    "            plt.axhline(y=2., xmin=0.0, xmax=51.0, linewidth=1, color='w')\n",
    "            plt.axhline(y=3., xmin=0.0, xmax=51.0, linewidth=1, color='w')\n",
    "\n",
    "        elif num_predictions == 1:\n",
    "            pylab.yticks([0.5], ['t+1'])\n",
    "            pylab.ylabel('y: t+1')\n",
    "\n",
    "        assert num_timesteps == 52\n",
    "\n",
    "        pylab.xticks([1.5, 11.5, 21.5, 31.5, 41.5, 51.5],\n",
    "                     ['t-50', 't-40', 't-30', 't-20', 't-10', 't'])\n",
    "        pylab.xlabel('x: t-51 to t')\n",
    "\n",
    "        pylab.title(f'{self.model_name} {title_supplement}')\n",
    "\n",
    "        mng = plt.get_current_fig_manager()\n",
    "        mng.window.showMaximized()\n",
    "        pylab.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
